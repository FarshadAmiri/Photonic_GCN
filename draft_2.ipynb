{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edge index excluded from dataloader (used it as a global variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os ; os.chdir(r\"D:\\Git_repos\\Photonic_GCN\")\n",
    "from constants import edges_table, nodes_groups\n",
    "# !pip install torch torch-geometric\n",
    "import torch , pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, random_split , DataLoader as DataLoader_primary\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "LR_SCHEDULER_GAMMA = 0.9\n",
    "LR_SCHEDULER_STEP_SIZE = 20\n",
    "EPOCHS = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edges list declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges_table_to_list(edges_table):\n",
    "    edges = []\n",
    "    for key in edges_table.keys():\n",
    "        nodes_list = edges_table[key]\n",
    "        for node in nodes_list:\n",
    "            if ((key, node) not in edges) and ((node, key) not in edges):\n",
    "                edges.append((key, node))\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n",
      "[('N69', 'N70'), ('N70', 'N71'), ('N71', 'N72'), ('N72', 'N73'), ('N73', 'N74'), ('N73', 'N60'), ('N74', 'N60'), ('N74', 'N94'), ('N60', 'N45'), ('N60', 'N46'), ('N60', 'N47'), ('N60', 'N61'), ('N60', 'N75'), ('N41', 'N42'), ('N42', 'N43'), ('N43', 'N44'), ('N44', 'N45'), ('N45', 'N46'), ('N46', 'N33'), ('N87', 'N86'), ('N86', 'N85'), ('N85', 'N84'), ('N84', 'N83'), ('N83', 'N82'), ('N83', 'N68'), ('N82', 'N68'), ('N82', 'N94'), ('N68', 'N55'), ('N68', 'N54'), ('N68', 'N53'), ('N68', 'N67'), ('N68', 'N81'), ('N59', 'N58'), ('N58', 'N57'), ('N57', 'N56'), ('N56', 'N55'), ('N55', 'N54'), ('N54', 'N34'), ('N33', 'N61'), ('N33', 'N47'), ('N33', 'N48'), ('N34', 'N52'), ('N34', 'N53'), ('N34', 'N67'), ('N94', 'N61'), ('N94', 'N75'), ('N94', 'N76'), ('N95', 'N80'), ('N95', 'N81'), ('N95', 'N82'), ('N95', 'N67'), ('N67', 'N81'), ('N67', 'N53'), ('N67', 'N80'), ('N67', 'N66'), ('N67', 'N52'), ('N53', 'N52'), ('N53', 'N66'), ('N81', 'N80'), ('N81', 'N66'), ('N80', 'N79'), ('N80', 'N66'), ('N80', 'N65'), ('N52', 'N51'), ('N52', 'N65'), ('N52', 'N66'), ('N79', 'N78'), ('N79', 'N64'), ('N79', 'N65'), ('N79', 'N66'), ('N51', 'N50'), ('N51', 'N64'), ('N51', 'N65'), ('N51', 'N66'), ('N65', 'N64'), ('N65', 'N78'), ('N65', 'N66'), ('N65', 'N50'), ('N78', 'N77'), ('N78', 'N63'), ('N78', 'N64'), ('N64', 'N63'), ('N64', 'N77'), ('N64', 'N49'), ('N64', 'N50'), ('N50', 'N49'), ('N50', 'N63'), ('N61', 'N75'), ('N61', 'N76'), ('N61', 'N62'), ('N61', 'N48'), ('N61', 'N47'), ('N75', 'N76'), ('N75', 'N62'), ('N47', 'N62'), ('N47', 'N48'), ('N62', 'N76'), ('N62', 'N77'), ('N62', 'N63'), ('N62', 'N49'), ('N62', 'N48'), ('N76', 'N77'), ('N76', 'N63'), ('N48', 'N63'), ('N48', 'N49'), ('N63', 'N77'), ('N63', 'N49')]\n"
     ]
    }
   ],
   "source": [
    "edge_list = edges_table_to_list(edges_table)  # 107 edges\n",
    "print(len(edge_list)) \n",
    "print(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N69', 'N70', 'N71', 'N72', 'N73', 'N74', 'N60', 'N41', 'N42', 'N43', 'N44', 'N45', 'N46', 'N87', 'N86', 'N85', 'N84', 'N83', 'N82', 'N68', 'N59', 'N58', 'N57', 'N56', 'N55', 'N54', 'N33', 'N34', 'N94', 'N95', 'N67', 'N53', 'N81', 'N80', 'N52', 'N79', 'N51', 'N65', 'N66', 'N78', 'N64', 'N50', 'N61', 'N75', 'N47', 'N62', 'N76', 'N48', 'N63', 'N77', 'N49']\n"
     ]
    }
   ],
   "source": [
    "node_names = list(edges_table.keys())  # 51 nodes\n",
    "print(node_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 107])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert edge list to indices\n",
    "edge_index = torch.tensor([[node_names.index(u), node_names.index(v)] for u, v in edge_list], dtype=torch.long).t().contiguous()\n",
    "edge_index_orignal = edge_index\n",
    "\n",
    "edge_index.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('data.xlsx')\n",
    "\n",
    "X = df.iloc[:, :18].values\n",
    "y = df.iloc[:, 18:19].values\n",
    "\n",
    "scaler_X, scaler_y = MinMaxScaler(), MinMaxScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y)\n",
    "\n",
    "X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)\n",
    "y_tensor = torch.tensor(y_scaled, dtype=torch.float32).to(device)\n",
    "\n",
    "train_size = int(0.7 * len(X_tensor))\n",
    "val_size = int(0.15 * len(X_tensor))\n",
    "test_size = len(X_tensor) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(TensorDataset(X_tensor, y_tensor), [train_size, val_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader_primary(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader_primary(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataloader = DataLoader_primary(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8015, 18)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dx1</th>\n",
       "      <th>dx2</th>\n",
       "      <th>dx3</th>\n",
       "      <th>dx4</th>\n",
       "      <th>dx5</th>\n",
       "      <th>dx6</th>\n",
       "      <th>dy1</th>\n",
       "      <th>dy2</th>\n",
       "      <th>dy3</th>\n",
       "      <th>dy4</th>\n",
       "      <th>...</th>\n",
       "      <th>dy6</th>\n",
       "      <th>dr1</th>\n",
       "      <th>dr2</th>\n",
       "      <th>dr3</th>\n",
       "      <th>dr4</th>\n",
       "      <th>dr5</th>\n",
       "      <th>dr6</th>\n",
       "      <th>Q</th>\n",
       "      <th>t_q_max</th>\n",
       "      <th>lambda_q_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.500000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>918.130324</td>\n",
       "      <td>0.554622</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.500000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.750000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>1846.114296</td>\n",
       "      <td>0.682024</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.500000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>3.000000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>660.817503</td>\n",
       "      <td>1.089219</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.500000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.750000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>1846.114296</td>\n",
       "      <td>0.682024</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.500000e-08</td>\n",
       "      <td>3.500000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.750000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>1846.114296</td>\n",
       "      <td>0.682024</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8010</th>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>3.500000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>1204.550299</td>\n",
       "      <td>0.521705</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8011</th>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>1306.557266</td>\n",
       "      <td>0.549646</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8012</th>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>4.500000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>618.706366</td>\n",
       "      <td>0.701339</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8013</th>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>5.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>610.208722</td>\n",
       "      <td>0.826126</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8014</th>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>5.500000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>-4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>2.500000e-08</td>\n",
       "      <td>597.357924</td>\n",
       "      <td>0.945110</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8015 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               dx1           dx2           dx3           dx4           dx5  \\\n",
       "0    -4.000000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08   \n",
       "1    -4.000000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08   \n",
       "2    -4.000000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08   \n",
       "3    -4.000000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08   \n",
       "4    -4.000000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "8010 -4.000000e-08  3.500000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08   \n",
       "8011 -4.000000e-08  4.000000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08   \n",
       "8012 -4.000000e-08  4.500000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08   \n",
       "8013 -4.000000e-08  5.000000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08   \n",
       "8014 -4.000000e-08  5.500000e-08 -4.000000e-08 -4.000000e-08 -4.000000e-08   \n",
       "\n",
       "               dx6           dy1           dy2           dy3           dy4  \\\n",
       "0    -4.500000e-08  4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08   \n",
       "1    -4.500000e-08  4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08   \n",
       "2    -4.500000e-08  4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08   \n",
       "3    -4.500000e-08  4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08   \n",
       "4    -4.500000e-08  3.500000e-08  4.000000e-08  4.000000e-08  4.000000e-08   \n",
       "...            ...           ...           ...           ...           ...   \n",
       "8010 -4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08   \n",
       "8011 -4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08   \n",
       "8012 -4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08   \n",
       "8013 -4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08   \n",
       "8014 -4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08  4.000000e-08   \n",
       "\n",
       "      ...           dy6           dr1           dr2           dr3  \\\n",
       "0     ...  4.000000e-08  2.500000e-08  2.500000e-08  2.500000e-08   \n",
       "1     ...  4.000000e-08  2.500000e-08  2.500000e-08  2.500000e-08   \n",
       "2     ...  4.000000e-08  2.500000e-08  2.500000e-08  2.500000e-08   \n",
       "3     ...  4.000000e-08  2.500000e-08  2.500000e-08  2.500000e-08   \n",
       "4     ...  4.000000e-08  2.500000e-08  2.500000e-08  2.500000e-08   \n",
       "...   ...           ...           ...           ...           ...   \n",
       "8010  ...  4.000000e-08  2.500000e-08  2.500000e-08  2.500000e-08   \n",
       "8011  ...  4.000000e-08  2.500000e-08  2.500000e-08  2.500000e-08   \n",
       "8012  ...  4.000000e-08  2.500000e-08  2.500000e-08  2.500000e-08   \n",
       "8013  ...  4.000000e-08  2.500000e-08  2.500000e-08  2.500000e-08   \n",
       "8014  ...  4.000000e-08  2.500000e-08  2.500000e-08  2.500000e-08   \n",
       "\n",
       "               dr4           dr5           dr6            Q   t_q_max  \\\n",
       "0     2.500000e-08  2.500000e-08  2.500000e-08   918.130324  0.554622   \n",
       "1     2.500000e-08  2.750000e-08  2.500000e-08  1846.114296  0.682024   \n",
       "2     2.500000e-08  3.000000e-08  2.500000e-08   660.817503  1.089219   \n",
       "3     2.500000e-08  2.750000e-08  2.500000e-08  1846.114296  0.682024   \n",
       "4     2.500000e-08  2.750000e-08  2.500000e-08  1846.114296  0.682024   \n",
       "...            ...           ...           ...          ...       ...   \n",
       "8010  2.500000e-08  2.500000e-08  2.500000e-08  1204.550299  0.521705   \n",
       "8011  2.500000e-08  2.500000e-08  2.500000e-08  1306.557266  0.549646   \n",
       "8012  2.500000e-08  2.500000e-08  2.500000e-08   618.706366  0.701339   \n",
       "8013  2.500000e-08  2.500000e-08  2.500000e-08   610.208722  0.826126   \n",
       "8014  2.500000e-08  2.500000e-08  2.500000e-08   597.357924  0.945110   \n",
       "\n",
       "      lambda_q_max  \n",
       "0         0.000001  \n",
       "1         0.000001  \n",
       "2         0.000002  \n",
       "3         0.000001  \n",
       "4         0.000001  \n",
       "...            ...  \n",
       "8010      0.000001  \n",
       "8011      0.000001  \n",
       "8012      0.000002  \n",
       "8013      0.000002  \n",
       "8014      0.000002  \n",
       "\n",
       "[8015 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes2group = dict()\n",
    "\n",
    "for g in nodes_groups:\n",
    "    for node in nodes_groups[g]:\n",
    "        nodes2group[node] = g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dx , dy are between 1.55e-7 and -4e-8\n",
    "\n",
    "dr is between 1.75e-8 and 3.25e-8 \n",
    "\n",
    "Almost 98% of Qs are between 500 and 50000\n",
    "\n",
    "we multiply dx, dy & dr by 1e+7 and Q by 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dx1</th>\n",
       "      <th>dx2</th>\n",
       "      <th>dx3</th>\n",
       "      <th>dx4</th>\n",
       "      <th>dx5</th>\n",
       "      <th>dx6</th>\n",
       "      <th>dy1</th>\n",
       "      <th>dy2</th>\n",
       "      <th>dy3</th>\n",
       "      <th>dy4</th>\n",
       "      <th>...</th>\n",
       "      <th>dy6</th>\n",
       "      <th>dr1</th>\n",
       "      <th>dr2</th>\n",
       "      <th>dr3</th>\n",
       "      <th>dr4</th>\n",
       "      <th>dr5</th>\n",
       "      <th>dr6</th>\n",
       "      <th>Q</th>\n",
       "      <th>t_q_max</th>\n",
       "      <th>lambda_q_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.091813</td>\n",
       "      <td>0.554622</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.184611</td>\n",
       "      <td>0.682024</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.300</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.066082</td>\n",
       "      <td>1.089219</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.184611</td>\n",
       "      <td>0.682024</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.184611</td>\n",
       "      <td>0.682024</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8010</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.120455</td>\n",
       "      <td>0.521705</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8011</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.130656</td>\n",
       "      <td>0.549646</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8012</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.061871</td>\n",
       "      <td>0.701339</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8013</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.061021</td>\n",
       "      <td>0.826126</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8014</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.059736</td>\n",
       "      <td>0.945110</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8015 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dx1   dx2  dx3  dx4  dx5   dx6   dy1  dy2  dy3  dy4  ...  dy6   dr1  \\\n",
       "0    -0.4 -0.40 -0.4 -0.4 -0.4 -0.45  0.40  0.4  0.4  0.4  ...  0.4  0.25   \n",
       "1    -0.4 -0.40 -0.4 -0.4 -0.4 -0.45  0.40  0.4  0.4  0.4  ...  0.4  0.25   \n",
       "2    -0.4 -0.40 -0.4 -0.4 -0.4 -0.45  0.40  0.4  0.4  0.4  ...  0.4  0.25   \n",
       "3    -0.4 -0.40 -0.4 -0.4 -0.4 -0.45  0.40  0.4  0.4  0.4  ...  0.4  0.25   \n",
       "4    -0.4 -0.40 -0.4 -0.4 -0.4 -0.45  0.35  0.4  0.4  0.4  ...  0.4  0.25   \n",
       "...   ...   ...  ...  ...  ...   ...   ...  ...  ...  ...  ...  ...   ...   \n",
       "8010 -0.4  0.35 -0.4 -0.4 -0.4 -0.40  0.40  0.4  0.4  0.4  ...  0.4  0.25   \n",
       "8011 -0.4  0.40 -0.4 -0.4 -0.4 -0.40  0.40  0.4  0.4  0.4  ...  0.4  0.25   \n",
       "8012 -0.4  0.45 -0.4 -0.4 -0.4 -0.40  0.40  0.4  0.4  0.4  ...  0.4  0.25   \n",
       "8013 -0.4  0.50 -0.4 -0.4 -0.4 -0.40  0.40  0.4  0.4  0.4  ...  0.4  0.25   \n",
       "8014 -0.4  0.55 -0.4 -0.4 -0.4 -0.40  0.40  0.4  0.4  0.4  ...  0.4  0.25   \n",
       "\n",
       "       dr2   dr3   dr4    dr5   dr6         Q   t_q_max  lambda_q_max  \n",
       "0     0.25  0.25  0.25  0.250  0.25  0.091813  0.554622      0.000001  \n",
       "1     0.25  0.25  0.25  0.275  0.25  0.184611  0.682024      0.000001  \n",
       "2     0.25  0.25  0.25  0.300  0.25  0.066082  1.089219      0.000002  \n",
       "3     0.25  0.25  0.25  0.275  0.25  0.184611  0.682024      0.000001  \n",
       "4     0.25  0.25  0.25  0.275  0.25  0.184611  0.682024      0.000001  \n",
       "...    ...   ...   ...    ...   ...       ...       ...           ...  \n",
       "8010  0.25  0.25  0.25  0.250  0.25  0.120455  0.521705      0.000001  \n",
       "8011  0.25  0.25  0.25  0.250  0.25  0.130656  0.549646      0.000001  \n",
       "8012  0.25  0.25  0.25  0.250  0.25  0.061871  0.701339      0.000002  \n",
       "8013  0.25  0.25  0.25  0.250  0.25  0.061021  0.826126      0.000002  \n",
       "8014  0.25  0.25  0.25  0.250  0.25  0.059736  0.945110      0.000002  \n",
       "\n",
       "[8015 rows x 21 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_df = df.copy()\n",
    "\n",
    "normalized_df.iloc[:, 0:18] = normalized_df.iloc[:, 0:18] * 1e+7\n",
    "normalized_df.iloc[:, 18] = normalized_df.iloc[:, 18] * 1e-4\n",
    "normalized_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dx1</th>\n",
       "      <th>dx2</th>\n",
       "      <th>dx3</th>\n",
       "      <th>dx4</th>\n",
       "      <th>dx5</th>\n",
       "      <th>dx6</th>\n",
       "      <th>dy1</th>\n",
       "      <th>dy2</th>\n",
       "      <th>dy3</th>\n",
       "      <th>dy4</th>\n",
       "      <th>...</th>\n",
       "      <th>dy6</th>\n",
       "      <th>dr1</th>\n",
       "      <th>dr2</th>\n",
       "      <th>dr3</th>\n",
       "      <th>dr4</th>\n",
       "      <th>dr5</th>\n",
       "      <th>dr6</th>\n",
       "      <th>Q</th>\n",
       "      <th>t_q_max</th>\n",
       "      <th>lambda_q_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6438</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.139398</td>\n",
       "      <td>0.515356</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.123603</td>\n",
       "      <td>0.691624</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.068667</td>\n",
       "      <td>1.086635</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.067726</td>\n",
       "      <td>0.710172</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4962</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.153093</td>\n",
       "      <td>0.573313</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5226</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.20</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.047967</td>\n",
       "      <td>1.072468</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5390</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.20</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.117560</td>\n",
       "      <td>0.513500</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.054203</td>\n",
       "      <td>0.900257</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7603</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.123656</td>\n",
       "      <td>0.697853</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7270</th>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.063797</td>\n",
       "      <td>0.868344</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8015 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      dx1   dx2  dx3   dx4   dx5   dx6   dy1  dy2   dy3   dy4  ...   dy6  \\\n",
       "6438 -0.4  0.40 -0.4 -0.40 -0.40 -0.40  0.40  0.4  0.40  0.35  ...  0.40   \n",
       "318  -0.4 -0.25 -0.4 -0.40 -0.45 -0.40  0.40  0.4  0.45  0.40  ...  0.10   \n",
       "1882 -0.4 -0.25 -0.4 -0.40 -0.35 -0.40  0.35  0.4  0.40  0.40  ...  0.40   \n",
       "5166 -0.4  0.15 -0.4 -0.40 -0.40 -0.40  0.40  0.4  0.35  0.40  ...  0.40   \n",
       "4962 -0.4  0.35 -0.4 -0.40 -0.40 -0.40  0.40  0.4  0.40  0.40  ...  0.40   \n",
       "...   ...   ...  ...   ...   ...   ...   ...  ...   ...   ...  ...   ...   \n",
       "5226 -0.4  1.20 -0.4 -0.40 -0.40 -0.40  0.40  0.4  0.40  0.40  ...  0.40   \n",
       "5390 -0.4  1.20 -0.4 -0.40 -0.40 -0.40  0.40  0.4  0.40  0.40  ...  0.40   \n",
       "860  -0.4 -0.40 -0.4 -0.45 -0.40 -0.40  0.40  0.4  0.40  0.35  ...  0.15   \n",
       "7603 -0.4  0.90 -0.4 -0.40 -0.40 -0.40  0.45  0.4  0.40  0.40  ...  0.40   \n",
       "7270 -0.4  0.45 -0.4 -0.40 -0.40 -0.35  0.40  0.4  0.40  0.30  ...  0.40   \n",
       "\n",
       "       dr1    dr2   dr3    dr4    dr5    dr6         Q   t_q_max  lambda_q_max  \n",
       "6438  0.25  0.250  0.25  0.250  0.250  0.225  0.139398  0.515356      0.000001  \n",
       "318   0.25  0.275  0.25  0.225  0.275  0.225  0.123603  0.691624      0.000002  \n",
       "1882  0.25  0.250  0.25  0.250  0.250  0.225  0.068667  1.086635      0.000001  \n",
       "5166  0.25  0.250  0.25  0.250  0.250  0.250  0.067726  0.710172      0.000002  \n",
       "4962  0.25  0.250  0.25  0.250  0.250  0.250  0.153093  0.573313      0.000001  \n",
       "...    ...    ...   ...    ...    ...    ...       ...       ...           ...  \n",
       "5226  0.25  0.250  0.25  0.250  0.250  0.250  0.047967  1.072468      0.000002  \n",
       "5390  0.25  0.275  0.25  0.250  0.250  0.250  0.117560  0.513500      0.000001  \n",
       "860   0.25  0.250  0.25  0.225  0.250  0.250  0.054203  0.900257      0.000001  \n",
       "7603  0.25  0.250  0.25  0.250  0.250  0.250  0.123656  0.697853      0.000001  \n",
       "7270  0.25  0.250  0.25  0.250  0.250  0.250  0.063797  0.868344      0.000002  \n",
       "\n",
       "[8015 rows x 21 columns]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "shuffled_df = normalized_df.sample(frac=1)\n",
    "shuffled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indice_split = 7600\n",
    "\n",
    "train_df = shuffled_df[:train_indice_split].reset_index()\n",
    "test_df = shuffled_df[train_indice_split:].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>dx1</th>\n",
       "      <th>dx2</th>\n",
       "      <th>dx3</th>\n",
       "      <th>dx4</th>\n",
       "      <th>dx5</th>\n",
       "      <th>dx6</th>\n",
       "      <th>dy1</th>\n",
       "      <th>dy2</th>\n",
       "      <th>dy3</th>\n",
       "      <th>...</th>\n",
       "      <th>dy6</th>\n",
       "      <th>dr1</th>\n",
       "      <th>dr2</th>\n",
       "      <th>dr3</th>\n",
       "      <th>dr4</th>\n",
       "      <th>dr5</th>\n",
       "      <th>dr6</th>\n",
       "      <th>Q</th>\n",
       "      <th>t_q_max</th>\n",
       "      <th>lambda_q_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6438</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.139398</td>\n",
       "      <td>0.515356</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>318</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.123603</td>\n",
       "      <td>0.691624</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1882</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.068667</td>\n",
       "      <td>1.086635</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5166</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.067726</td>\n",
       "      <td>0.710172</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4962</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.153093</td>\n",
       "      <td>0.573313</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7595</th>\n",
       "      <td>4735</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.15</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.048688</td>\n",
       "      <td>0.866636</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7596</th>\n",
       "      <td>7713</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.134603</td>\n",
       "      <td>0.606360</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7597</th>\n",
       "      <td>1218</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.091626</td>\n",
       "      <td>0.551384</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7598</th>\n",
       "      <td>5254</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.056174</td>\n",
       "      <td>0.851991</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7599</th>\n",
       "      <td>5511</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.20</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.047967</td>\n",
       "      <td>1.072468</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7600 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  dx1   dx2   dx3   dx4   dx5  dx6   dy1  dy2   dy3  ...  dy6  \\\n",
       "0      6438 -0.4  0.40 -0.40 -0.40 -0.40 -0.4  0.40  0.4  0.40  ...  0.4   \n",
       "1       318 -0.4 -0.25 -0.40 -0.40 -0.45 -0.4  0.40  0.4  0.45  ...  0.1   \n",
       "2      1882 -0.4 -0.25 -0.40 -0.40 -0.35 -0.4  0.35  0.4  0.40  ...  0.4   \n",
       "3      5166 -0.4  0.15 -0.40 -0.40 -0.40 -0.4  0.40  0.4  0.35  ...  0.4   \n",
       "4      4962 -0.4  0.35 -0.40 -0.40 -0.40 -0.4  0.40  0.4  0.40  ...  0.4   \n",
       "...     ...  ...   ...   ...   ...   ...  ...   ...  ...   ...  ...  ...   \n",
       "7595   4735 -0.4  1.15 -0.45 -0.40 -0.40 -0.4  0.40  0.4  0.40  ...  0.4   \n",
       "7596   7713 -0.4  0.15 -0.40 -0.40 -0.40 -0.4  0.40  0.4  0.40  ...  0.4   \n",
       "7597   1218 -0.4  1.45 -0.40 -0.35 -0.35 -0.4  0.35  0.4  0.40  ...  0.3   \n",
       "7598   5254 -0.4  0.55 -0.40 -0.40 -0.35 -0.4  0.40  0.4  0.40  ...  0.4   \n",
       "7599   5511 -0.4  1.20 -0.40 -0.40 -0.40 -0.4  0.40  0.4  0.40  ...  0.4   \n",
       "\n",
       "       dr1    dr2    dr3    dr4    dr5    dr6         Q   t_q_max  \\\n",
       "0     0.25  0.250  0.250  0.250  0.250  0.225  0.139398  0.515356   \n",
       "1     0.25  0.275  0.250  0.225  0.275  0.225  0.123603  0.691624   \n",
       "2     0.25  0.250  0.250  0.250  0.250  0.225  0.068667  1.086635   \n",
       "3     0.25  0.250  0.250  0.250  0.250  0.250  0.067726  0.710172   \n",
       "4     0.25  0.250  0.250  0.250  0.250  0.250  0.153093  0.573313   \n",
       "...    ...    ...    ...    ...    ...    ...       ...       ...   \n",
       "7595  0.25  0.250  0.250  0.250  0.250  0.250  0.048688  0.866636   \n",
       "7596  0.25  0.250  0.250  0.250  0.225  0.250  0.134603  0.606360   \n",
       "7597  0.25  0.225  0.225  0.275  0.250  0.250  0.091626  0.551384   \n",
       "7598  0.25  0.250  0.250  0.250  0.250  0.250  0.056174  0.851991   \n",
       "7599  0.25  0.250  0.250  0.250  0.250  0.250  0.047967  1.072468   \n",
       "\n",
       "      lambda_q_max  \n",
       "0         0.000001  \n",
       "1         0.000002  \n",
       "2         0.000001  \n",
       "3         0.000002  \n",
       "4         0.000001  \n",
       "...            ...  \n",
       "7595      0.000002  \n",
       "7596      0.000001  \n",
       "7597      0.000001  \n",
       "7598      0.000002  \n",
       "7599      0.000002  \n",
       "\n",
       "[7600 rows x 22 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>dx1</th>\n",
       "      <th>dx2</th>\n",
       "      <th>dx3</th>\n",
       "      <th>dx4</th>\n",
       "      <th>dx5</th>\n",
       "      <th>dx6</th>\n",
       "      <th>dy1</th>\n",
       "      <th>dy2</th>\n",
       "      <th>dy3</th>\n",
       "      <th>...</th>\n",
       "      <th>dy6</th>\n",
       "      <th>dr1</th>\n",
       "      <th>dr2</th>\n",
       "      <th>dr3</th>\n",
       "      <th>dr4</th>\n",
       "      <th>dr5</th>\n",
       "      <th>dr6</th>\n",
       "      <th>Q</th>\n",
       "      <th>t_q_max</th>\n",
       "      <th>lambda_q_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7618</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.45</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.047586</td>\n",
       "      <td>0.861553</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1527</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.075180</td>\n",
       "      <td>0.602837</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3987</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.137036</td>\n",
       "      <td>0.527903</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7906</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.15</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.095046</td>\n",
       "      <td>0.557288</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1300</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.131589</td>\n",
       "      <td>0.525373</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410</th>\n",
       "      <td>5226</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.20</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.047967</td>\n",
       "      <td>1.072468</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>411</th>\n",
       "      <td>5390</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>1.20</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.117560</td>\n",
       "      <td>0.513500</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>412</th>\n",
       "      <td>860</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.45</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.225</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.054203</td>\n",
       "      <td>0.900257</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>7603</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.123656</td>\n",
       "      <td>0.697853</td>\n",
       "      <td>0.000001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>7270</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.063797</td>\n",
       "      <td>0.868344</td>\n",
       "      <td>0.000002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>415 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index  dx1   dx2  dx3   dx4  dx5   dx6   dy1   dy2   dy3  ...   dy6  \\\n",
       "0     7618 -0.4  1.45 -0.4 -0.40 -0.4 -0.45  0.45  0.35  0.40  ...  0.40   \n",
       "1     1527 -0.4 -0.40 -0.4 -0.40 -0.4 -0.40  0.40  0.40  0.40  ...  0.40   \n",
       "2     3987 -0.4  0.75 -0.4 -0.40 -0.4 -0.40  0.40  0.40  0.45  ...  0.40   \n",
       "3     7906 -0.4  1.15 -0.4 -0.40 -0.4 -0.40  0.40  0.35  0.40  ...  0.40   \n",
       "4     1300 -0.4 -0.40 -0.4 -0.40 -0.4 -0.40  0.40  0.40  0.40  ...  0.30   \n",
       "..     ...  ...   ...  ...   ...  ...   ...   ...   ...   ...  ...   ...   \n",
       "410   5226 -0.4  1.20 -0.4 -0.40 -0.4 -0.40  0.40  0.40  0.40  ...  0.40   \n",
       "411   5390 -0.4  1.20 -0.4 -0.40 -0.4 -0.40  0.40  0.40  0.40  ...  0.40   \n",
       "412    860 -0.4 -0.40 -0.4 -0.45 -0.4 -0.40  0.40  0.40  0.40  ...  0.15   \n",
       "413   7603 -0.4  0.90 -0.4 -0.40 -0.4 -0.40  0.45  0.40  0.40  ...  0.40   \n",
       "414   7270 -0.4  0.45 -0.4 -0.40 -0.4 -0.35  0.40  0.40  0.40  ...  0.40   \n",
       "\n",
       "      dr1    dr2   dr3    dr4   dr5   dr6         Q   t_q_max  lambda_q_max  \n",
       "0    0.25  0.250  0.25  0.250  0.25  0.25  0.047586  0.861553      0.000002  \n",
       "1    0.25  0.250  0.25  0.250  0.25  0.25  0.075180  0.602837      0.000001  \n",
       "2    0.25  0.250  0.25  0.250  0.25  0.25  0.137036  0.527903      0.000001  \n",
       "3    0.25  0.250  0.25  0.250  0.25  0.25  0.095046  0.557288      0.000001  \n",
       "4    0.25  0.250  0.25  0.250  0.25  0.25  0.131589  0.525373      0.000001  \n",
       "..    ...    ...   ...    ...   ...   ...       ...       ...           ...  \n",
       "410  0.25  0.250  0.25  0.250  0.25  0.25  0.047967  1.072468      0.000002  \n",
       "411  0.25  0.275  0.25  0.250  0.25  0.25  0.117560  0.513500      0.000001  \n",
       "412  0.25  0.250  0.25  0.225  0.25  0.25  0.054203  0.900257      0.000001  \n",
       "413  0.25  0.250  0.25  0.250  0.25  0.25  0.123656  0.697853      0.000001  \n",
       "414  0.25  0.250  0.25  0.250  0.25  0.25  0.063797  0.868344      0.000002  \n",
       "\n",
       "[415 rows x 22 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7600, 51, 3])\n",
      "torch.Size([7600])\n",
      "torch.Size([2, 107])\n"
     ]
    }
   ],
   "source": [
    "train_x_list = []\n",
    "train_y_list = []\n",
    "\n",
    "for n in range(len(train_df)):\n",
    "    x = np.zeros((51, 3))\n",
    "    y = np.array(train_df[\"Q\"][n])\n",
    "    for idx, node in enumerate(node_names):\n",
    "        group = nodes2group[node][1:]\n",
    "        node_features = np.array((train_df[\"dx\" + group][n], train_df[\"dy\" + group][n], train_df[\"dr\" + group][n],))\n",
    "        x[idx] = node_features\n",
    "    train_x_list.append(x)\n",
    "    train_y_list.append(y)\n",
    "\n",
    "train_x = torch.from_numpy(np.stack(train_x_list, axis=0))\n",
    "train_y = torch.from_numpy(np.stack(train_y_list, axis=0))\n",
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([415, 51, 3])\n",
      "torch.Size([415])\n",
      "torch.Size([2, 107])\n"
     ]
    }
   ],
   "source": [
    "test_x_list = []\n",
    "test_y_list = []\n",
    "\n",
    "for n in range(len(test_df)):\n",
    "    x = np.zeros((51, 3))\n",
    "    y = np.array(test_df[\"Q\"][n])\n",
    "    for idx, node in enumerate(node_names):\n",
    "        group = nodes2group[node][1:]\n",
    "        node_features = np.array((test_df[\"dx\" + group][n], test_df[\"dy\" + group][n], test_df[\"dr\" + group][n],))\n",
    "        x[idx] = node_features\n",
    "    test_x_list.append(x)\n",
    "    test_y_list.append(y)\n",
    "\n",
    "test_x = torch.from_numpy(np.stack(test_x_list, axis=0))\n",
    "test_y = torch.from_numpy(np.stack(test_y_list, axis=0))\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)\n",
    "print(edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): Tensor of shape (8015, 51, 3) containing the input data.\n",
    "            labels (Tensor): Tensor of shape (8015,) containing the labels.\n",
    "        \"\"\"\n",
    "        self.x = x.float()  # Ensure x is float32\n",
    "        self.y = y.float()  # Ensure y is float32\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MyDataset(train_x, train_y)\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "dataset_test = MyDataset(test_x, test_y)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 1)  # Output 1 feature per node\n",
    "\n",
    "    def forward(self, data, edge_index):\n",
    "        x = data\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn = GCN(in_channels, hidden_channels)\n",
    "        self.fc1 = torch.nn.Linear(51, 128)  # Input size should match number of nodes\n",
    "        self.fc2 = torch.nn.Linear(128, 1)   # Output a single value\n",
    "\n",
    "    def forward(self, data, edge_index):\n",
    "        x = self.gcn(data, edge_index)\n",
    "        x = x.view(1, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "in_channels = 3\n",
    "hidden_channels = 16\n",
    "\n",
    "# Initialize model\n",
    "model = Net(in_channels, hidden_channels)\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = StepLR(optimizer, step_size=LR_SCHEDULER_STEP_SIZE, gamma=LR_SCHEDULER_GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.020333 *** Best loss so far: 100.000000 at epoch 1\n",
      "Epoch 2, Loss: 0.020047 *** Best loss so far: 0.020333 at epoch 1\n",
      "Epoch 3, Loss: 0.020034 *** Best loss so far: 0.020047 at epoch 2\n",
      "Epoch 4, Loss: 0.019932 *** Best loss so far: 0.020034 at epoch 3\n",
      "Epoch 5, Loss: 0.019908 *** Best loss so far: 0.019932 at epoch 4\n",
      "Epoch 6, Loss: 0.019843 *** Best loss so far: 0.019908 at epoch 5\n",
      "Epoch 7, Loss: 0.019897 *** Best loss so far: 0.019843 at epoch 6\n",
      "Epoch 8, Loss: 0.019835 *** Best loss so far: 0.019843 at epoch 6\n",
      "Epoch 9, Loss: 0.019868 *** Best loss so far: 0.019835 at epoch 8\n",
      "Epoch 10, Loss: 0.019700 *** Best loss so far: 0.019835 at epoch 8\n",
      "Epoch 11, Loss: 0.019810 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 12, Loss: 0.019821 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 13, Loss: 0.019843 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 14, Loss: 0.019876 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 15, Loss: 0.019783 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 16, Loss: 0.019815 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 17, Loss: 0.019746 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 18, Loss: 0.019825 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 19, Loss: 0.019704 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 20, Loss: 0.019773 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 21, Loss: 0.019756 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 22, Loss: 0.019709 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 23, Loss: 0.019716 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 24, Loss: 0.019689 *** Best loss so far: 0.019700 at epoch 10\n",
      "Epoch 25, Loss: 0.019617 *** Best loss so far: 0.019689 at epoch 24\n",
      "Epoch 26, Loss: 0.019619 *** Best loss so far: 0.019617 at epoch 25\n",
      "Epoch 27, Loss: 0.019695 *** Best loss so far: 0.019617 at epoch 25\n",
      "Epoch 28, Loss: 0.019643 *** Best loss so far: 0.019617 at epoch 25\n",
      "Epoch 29, Loss: 0.019642 *** Best loss so far: 0.019617 at epoch 25\n",
      "Epoch 30, Loss: 0.019614 *** Best loss so far: 0.019617 at epoch 25\n",
      "Epoch 31, Loss: 0.019524 *** Best loss so far: 0.019614 at epoch 30\n",
      "Epoch 32, Loss: 0.019697 *** Best loss so far: 0.019524 at epoch 31\n",
      "Epoch 33, Loss: 0.019639 *** Best loss so far: 0.019524 at epoch 31\n",
      "Epoch 34, Loss: 0.019659 *** Best loss so far: 0.019524 at epoch 31\n",
      "Epoch 35, Loss: 0.019735 *** Best loss so far: 0.019524 at epoch 31\n",
      "Epoch 36, Loss: 0.019614 *** Best loss so far: 0.019524 at epoch 31\n",
      "Epoch 37, Loss: 0.019586 *** Best loss so far: 0.019524 at epoch 31\n",
      "Epoch 38, Loss: 0.019614 *** Best loss so far: 0.019524 at epoch 31\n",
      "Epoch 39, Loss: 0.019525 *** Best loss so far: 0.019524 at epoch 31\n",
      "Epoch 40, Loss: 0.019455 *** Best loss so far: 0.019524 at epoch 31\n",
      "Epoch 41, Loss: 0.019670 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 42, Loss: 0.019583 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 43, Loss: 0.019643 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 44, Loss: 0.019649 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 45, Loss: 0.019560 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 46, Loss: 0.019734 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 47, Loss: 0.019525 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 48, Loss: 0.019721 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 49, Loss: 0.019562 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 50, Loss: 0.019607 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 51, Loss: 0.019509 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 52, Loss: 0.019561 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 53, Loss: 0.019505 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 54, Loss: 0.019604 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 55, Loss: 0.019534 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 56, Loss: 0.019547 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 57, Loss: 0.019507 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 58, Loss: 0.019542 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 59, Loss: 0.019614 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 60, Loss: 0.019448 *** Best loss so far: 0.019455 at epoch 40\n",
      "Epoch 61, Loss: 0.019828 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 62, Loss: 0.019772 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 63, Loss: 0.019778 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 64, Loss: 0.019585 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 65, Loss: 0.019578 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 66, Loss: 0.019554 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 67, Loss: 0.019643 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 68, Loss: 0.019448 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 69, Loss: 0.019545 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 70, Loss: 0.019545 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 71, Loss: 0.019559 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 72, Loss: 0.019559 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 73, Loss: 0.019538 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 74, Loss: 0.019542 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 75, Loss: 0.019513 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 76, Loss: 0.019574 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 77, Loss: 0.019459 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 78, Loss: 0.019438 *** Best loss so far: 0.019448 at epoch 60\n",
      "Epoch 79, Loss: 0.019488 *** Best loss so far: 0.019438 at epoch 78\n",
      "Epoch 80, Loss: 0.019509 *** Best loss so far: 0.019438 at epoch 78\n",
      "Epoch 81, Loss: 0.019471 *** Best loss so far: 0.019438 at epoch 78\n",
      "Epoch 82, Loss: 0.019535 *** Best loss so far: 0.019438 at epoch 78\n",
      "Epoch 83, Loss: 0.019440 *** Best loss so far: 0.019438 at epoch 78\n",
      "Epoch 84, Loss: 0.019425 *** Best loss so far: 0.019438 at epoch 78\n",
      "Epoch 85, Loss: 0.019580 *** Best loss so far: 0.019425 at epoch 84\n",
      "Epoch 86, Loss: 0.019401 *** Best loss so far: 0.019425 at epoch 84\n",
      "Epoch 87, Loss: 0.019378 *** Best loss so far: 0.019401 at epoch 86\n",
      "Epoch 88, Loss: 0.019447 *** Best loss so far: 0.019378 at epoch 87\n",
      "Epoch 89, Loss: 0.019483 *** Best loss so far: 0.019378 at epoch 87\n",
      "Epoch 90, Loss: 0.019597 *** Best loss so far: 0.019378 at epoch 87\n",
      "Epoch 91, Loss: 0.019437 *** Best loss so far: 0.019378 at epoch 87\n",
      "Epoch 92, Loss: 0.019403 *** Best loss so far: 0.019378 at epoch 87\n",
      "Epoch 93, Loss: 0.019535 *** Best loss so far: 0.019378 at epoch 87\n",
      "Epoch 94, Loss: 0.019604 *** Best loss so far: 0.019378 at epoch 87\n",
      "Epoch 95, Loss: 0.019426 *** Best loss so far: 0.019378 at epoch 87\n",
      "Epoch 96, Loss: 0.019376 *** Best loss so far: 0.019378 at epoch 87\n",
      "Epoch 97, Loss: 0.019395 *** Best loss so far: 0.019376 at epoch 96\n",
      "Epoch 98, Loss: 0.019336 *** Best loss so far: 0.019376 at epoch 96\n",
      "Epoch 99, Loss: 0.019395 *** Best loss so far: 0.019336 at epoch 98\n",
      "Epoch 100, Loss: 0.019384 *** Best loss so far: 0.019336 at epoch 98\n",
      "Epoch 101, Loss: 0.019284 *** Best loss so far: 0.019336 at epoch 98\n",
      "Epoch 102, Loss: 0.019321 *** Best loss so far: 0.019284 at epoch 101\n",
      "Epoch 103, Loss: 0.019336 *** Best loss so far: 0.019284 at epoch 101\n",
      "Epoch 104, Loss: 0.019272 *** Best loss so far: 0.019284 at epoch 101\n",
      "Epoch 105, Loss: 0.019407 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 106, Loss: 0.019298 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 107, Loss: 0.019395 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 108, Loss: 0.019490 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 109, Loss: 0.019293 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 110, Loss: 0.019390 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 111, Loss: 0.019383 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 112, Loss: 0.019393 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 113, Loss: 0.019369 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 114, Loss: 0.019461 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 115, Loss: 0.019309 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 116, Loss: 0.019407 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 117, Loss: 0.019625 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 118, Loss: 0.019355 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 119, Loss: 0.019420 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 120, Loss: 0.019396 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 121, Loss: 0.019365 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 122, Loss: 0.019309 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 123, Loss: 0.019240 *** Best loss so far: 0.019272 at epoch 104\n",
      "Epoch 124, Loss: 0.019363 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 125, Loss: 0.019541 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 126, Loss: 0.019263 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 127, Loss: 0.019249 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 128, Loss: 0.019611 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 129, Loss: 0.019594 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 130, Loss: 0.019399 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 131, Loss: 0.019308 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 132, Loss: 0.019323 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 133, Loss: 0.019355 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 134, Loss: 0.019246 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 135, Loss: 0.019344 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 136, Loss: 0.019504 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 137, Loss: 0.019444 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 138, Loss: 0.019321 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 139, Loss: 0.019295 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 140, Loss: 0.019214 *** Best loss so far: 0.019240 at epoch 123\n",
      "Epoch 141, Loss: 0.019509 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 142, Loss: 0.019614 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 143, Loss: 0.019316 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 144, Loss: 0.019325 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 145, Loss: 0.019284 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 146, Loss: 0.019275 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 147, Loss: 0.019310 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 148, Loss: 0.019325 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 149, Loss: 0.019343 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 150, Loss: 0.019376 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 151, Loss: 0.019315 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 152, Loss: 0.019344 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 153, Loss: 0.019301 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 154, Loss: 0.019207 *** Best loss so far: 0.019214 at epoch 140\n",
      "Epoch 155, Loss: 0.019329 *** Best loss so far: 0.019207 at epoch 154\n",
      "Epoch 156, Loss: 0.019288 *** Best loss so far: 0.019207 at epoch 154\n",
      "Epoch 157, Loss: 0.019231 *** Best loss so far: 0.019207 at epoch 154\n",
      "Epoch 158, Loss: 0.019297 *** Best loss so far: 0.019207 at epoch 154\n",
      "Epoch 159, Loss: 0.019236 *** Best loss so far: 0.019207 at epoch 154\n",
      "Epoch 160, Loss: 0.019177 *** Best loss so far: 0.019207 at epoch 154\n",
      "Epoch 161, Loss: 0.019239 *** Best loss so far: 0.019177 at epoch 160\n",
      "Epoch 162, Loss: 0.019249 *** Best loss so far: 0.019177 at epoch 160\n",
      "Epoch 163, Loss: 0.019188 *** Best loss so far: 0.019177 at epoch 160\n",
      "Epoch 164, Loss: 0.019245 *** Best loss so far: 0.019177 at epoch 160\n",
      "Epoch 165, Loss: 0.019225 *** Best loss so far: 0.019177 at epoch 160\n",
      "Epoch 166, Loss: 0.019255 *** Best loss so far: 0.019177 at epoch 160\n",
      "Epoch 167, Loss: 0.019226 *** Best loss so far: 0.019177 at epoch 160\n",
      "Epoch 168, Loss: 0.019244 *** Best loss so far: 0.019177 at epoch 160\n",
      "Epoch 169, Loss: 0.019257 *** Best loss so far: 0.019177 at epoch 160\n",
      "Epoch 170, Loss: 0.019161 *** Best loss so far: 0.019177 at epoch 160\n",
      "Epoch 171, Loss: 0.019269 *** Best loss so far: 0.019161 at epoch 170\n",
      "Epoch 172, Loss: 0.019217 *** Best loss so far: 0.019161 at epoch 170\n",
      "Epoch 173, Loss: 0.019276 *** Best loss so far: 0.019161 at epoch 170\n",
      "Epoch 174, Loss: 0.019190 *** Best loss so far: 0.019161 at epoch 170\n",
      "Epoch 175, Loss: 0.019245 *** Best loss so far: 0.019161 at epoch 170\n",
      "Epoch 176, Loss: 0.019213 *** Best loss so far: 0.019161 at epoch 170\n",
      "Epoch 177, Loss: 0.019269 *** Best loss so far: 0.019161 at epoch 170\n",
      "Epoch 178, Loss: 0.019216 *** Best loss so far: 0.019161 at epoch 170\n",
      "Epoch 179, Loss: 0.019209 *** Best loss so far: 0.019161 at epoch 170\n",
      "Epoch 180, Loss: 0.019148 *** Best loss so far: 0.019161 at epoch 170\n",
      "Epoch 181, Loss: 0.019247 *** Best loss so far: 0.019148 at epoch 180\n",
      "Epoch 182, Loss: 0.019181 *** Best loss so far: 0.019148 at epoch 180\n",
      "Epoch 183, Loss: 0.019256 *** Best loss so far: 0.019148 at epoch 180\n",
      "Epoch 184, Loss: 0.019180 *** Best loss so far: 0.019148 at epoch 180\n",
      "Epoch 185, Loss: 0.019163 *** Best loss so far: 0.019148 at epoch 180\n",
      "Epoch 186, Loss: 0.019116 *** Best loss so far: 0.019148 at epoch 180\n",
      "Epoch 187, Loss: 0.019209 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 188, Loss: 0.019211 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 189, Loss: 0.019145 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 190, Loss: 0.019173 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 191, Loss: 0.019176 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 192, Loss: 0.019225 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 193, Loss: 0.019236 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 194, Loss: 0.019257 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 195, Loss: 0.019286 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 196, Loss: 0.019243 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 197, Loss: 0.019242 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 198, Loss: 0.019152 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 199, Loss: 0.019141 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 200, Loss: 0.019263 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 201, Loss: 0.019184 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 202, Loss: 0.019085 *** Best loss so far: 0.019116 at epoch 186\n",
      "Epoch 203, Loss: 0.019133 *** Best loss so far: 0.019085 at epoch 202\n",
      "Epoch 204, Loss: 0.019161 *** Best loss so far: 0.019085 at epoch 202\n",
      "Epoch 205, Loss: 0.019087 *** Best loss so far: 0.019085 at epoch 202\n",
      "Epoch 206, Loss: 0.019136 *** Best loss so far: 0.019085 at epoch 202\n",
      "Epoch 207, Loss: 0.019249 *** Best loss so far: 0.019085 at epoch 202\n",
      "Epoch 208, Loss: 0.019177 *** Best loss so far: 0.019085 at epoch 202\n",
      "Epoch 209, Loss: 0.019140 *** Best loss so far: 0.019085 at epoch 202\n",
      "Epoch 210, Loss: 0.019176 *** Best loss so far: 0.019085 at epoch 202\n",
      "Epoch 211, Loss: 0.018968 *** Best loss so far: 0.019085 at epoch 202\n",
      "Epoch 212, Loss: 0.019220 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 213, Loss: 0.019192 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 214, Loss: 0.019046 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 215, Loss: 0.019217 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 216, Loss: 0.019193 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 217, Loss: 0.019101 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 218, Loss: 0.019141 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 219, Loss: 0.019162 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 220, Loss: 0.019067 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 221, Loss: 0.019217 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 222, Loss: 0.019174 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 223, Loss: 0.019138 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 224, Loss: 0.019140 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 225, Loss: 0.019073 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 226, Loss: 0.019146 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 227, Loss: 0.019208 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 228, Loss: 0.019103 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 229, Loss: 0.019181 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 230, Loss: 0.019112 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 231, Loss: 0.019119 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 232, Loss: 0.019157 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 233, Loss: 0.019128 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 234, Loss: 0.019094 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 235, Loss: 0.019176 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 236, Loss: 0.019184 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 237, Loss: 0.019191 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 238, Loss: 0.019159 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 239, Loss: 0.019068 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 240, Loss: 0.019263 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 241, Loss: 0.019128 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 242, Loss: 0.019092 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 243, Loss: 0.019146 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 244, Loss: 0.019142 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 245, Loss: 0.019069 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 246, Loss: 0.019123 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 247, Loss: 0.019043 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 248, Loss: 0.019146 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 249, Loss: 0.019019 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 250, Loss: 0.019161 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 251, Loss: 0.019124 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 252, Loss: 0.019096 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 253, Loss: 0.019035 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 254, Loss: 0.019049 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 255, Loss: 0.019045 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 256, Loss: 0.019119 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 257, Loss: 0.018998 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 258, Loss: 0.019073 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 259, Loss: 0.019092 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 260, Loss: 0.019141 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 261, Loss: 0.019057 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 262, Loss: 0.019126 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 263, Loss: 0.019001 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 264, Loss: 0.019064 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 265, Loss: 0.019060 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 266, Loss: 0.019151 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 267, Loss: 0.019036 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 268, Loss: 0.019084 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 269, Loss: 0.019055 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 270, Loss: 0.018972 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 271, Loss: 0.019063 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 272, Loss: 0.019216 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 273, Loss: 0.019076 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 274, Loss: 0.019006 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 275, Loss: 0.019112 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 276, Loss: 0.019091 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 277, Loss: 0.019079 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 278, Loss: 0.018978 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 279, Loss: 0.019098 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 280, Loss: 0.019103 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 281, Loss: 0.019124 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 282, Loss: 0.018987 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 283, Loss: 0.019078 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 284, Loss: 0.019054 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 285, Loss: 0.019099 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 286, Loss: 0.019106 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 287, Loss: 0.019006 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 288, Loss: 0.019041 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 289, Loss: 0.018951 *** Best loss so far: 0.018968 at epoch 211\n",
      "Epoch 290, Loss: 0.019013 *** Best loss so far: 0.018951 at epoch 289\n",
      "Epoch 291, Loss: 0.018958 *** Best loss so far: 0.018951 at epoch 289\n",
      "Epoch 292, Loss: 0.019061 *** Best loss so far: 0.018951 at epoch 289\n",
      "Epoch 293, Loss: 0.018957 *** Best loss so far: 0.018951 at epoch 289\n",
      "Epoch 294, Loss: 0.019067 *** Best loss so far: 0.018951 at epoch 289\n",
      "Epoch 295, Loss: 0.018986 *** Best loss so far: 0.018951 at epoch 289\n",
      "Epoch 296, Loss: 0.018961 *** Best loss so far: 0.018951 at epoch 289\n",
      "Epoch 297, Loss: 0.018992 *** Best loss so far: 0.018951 at epoch 289\n",
      "Epoch 298, Loss: 0.018944 *** Best loss so far: 0.018951 at epoch 289\n",
      "Epoch 299, Loss: 0.019082 *** Best loss so far: 0.018944 at epoch 298\n",
      "Epoch 300, Loss: 0.019011 *** Best loss so far: 0.018944 at epoch 298\n",
      "Epoch 301, Loss: 0.018946 *** Best loss so far: 0.018944 at epoch 298\n",
      "Epoch 302, Loss: 0.019056 *** Best loss so far: 0.018944 at epoch 298\n",
      "Epoch 303, Loss: 0.018969 *** Best loss so far: 0.018944 at epoch 298\n",
      "Epoch 304, Loss: 0.018953 *** Best loss so far: 0.018944 at epoch 298\n",
      "Epoch 305, Loss: 0.018959 *** Best loss so far: 0.018944 at epoch 298\n",
      "Epoch 306, Loss: 0.018981 *** Best loss so far: 0.018944 at epoch 298\n",
      "Epoch 307, Loss: 0.018986 *** Best loss so far: 0.018944 at epoch 298\n",
      "Epoch 308, Loss: 0.018985 *** Best loss so far: 0.018944 at epoch 298\n",
      "Epoch 309, Loss: 0.018832 *** Best loss so far: 0.018944 at epoch 298\n",
      "Epoch 310, Loss: 0.018866 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 311, Loss: 0.018952 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 312, Loss: 0.019007 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 313, Loss: 0.018952 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 314, Loss: 0.018930 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 315, Loss: 0.018969 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 316, Loss: 0.019000 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 317, Loss: 0.018904 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 318, Loss: 0.019010 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 319, Loss: 0.018858 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 320, Loss: 0.018983 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 321, Loss: 0.018978 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 322, Loss: 0.019022 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 323, Loss: 0.018928 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 324, Loss: 0.018907 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 325, Loss: 0.018869 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 326, Loss: 0.019043 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 327, Loss: 0.018931 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 328, Loss: 0.018854 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 329, Loss: 0.018917 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 330, Loss: 0.018966 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 331, Loss: 0.018874 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 332, Loss: 0.018945 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 333, Loss: 0.018980 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 334, Loss: 0.018866 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 335, Loss: 0.018853 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 336, Loss: 0.018909 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 337, Loss: 0.018943 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 338, Loss: 0.018939 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 339, Loss: 0.018987 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 340, Loss: 0.018938 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 341, Loss: 0.018901 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 342, Loss: 0.018907 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 343, Loss: 0.018926 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 344, Loss: 0.018854 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 345, Loss: 0.018831 *** Best loss so far: 0.018832 at epoch 309\n",
      "Epoch 346, Loss: 0.018917 *** Best loss so far: 0.018831 at epoch 345\n",
      "Epoch 347, Loss: 0.018843 *** Best loss so far: 0.018831 at epoch 345\n",
      "Epoch 348, Loss: 0.018935 *** Best loss so far: 0.018831 at epoch 345\n",
      "Epoch 349, Loss: 0.018815 *** Best loss so far: 0.018831 at epoch 345\n",
      "Epoch 350, Loss: 0.018955 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 351, Loss: 0.018881 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 352, Loss: 0.018866 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 353, Loss: 0.018873 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 354, Loss: 0.018851 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 355, Loss: 0.018840 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 356, Loss: 0.018957 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 357, Loss: 0.018918 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 358, Loss: 0.018832 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 359, Loss: 0.018868 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 360, Loss: 0.018875 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 361, Loss: 0.018869 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 362, Loss: 0.018848 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 363, Loss: 0.018825 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 364, Loss: 0.018861 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 365, Loss: 0.018776 *** Best loss so far: 0.018815 at epoch 349\n",
      "Epoch 366, Loss: 0.018860 *** Best loss so far: 0.018776 at epoch 365\n",
      "Epoch 367, Loss: 0.018788 *** Best loss so far: 0.018776 at epoch 365\n",
      "Epoch 368, Loss: 0.018841 *** Best loss so far: 0.018776 at epoch 365\n",
      "Epoch 369, Loss: 0.018877 *** Best loss so far: 0.018776 at epoch 365\n",
      "Epoch 370, Loss: 0.018752 *** Best loss so far: 0.018776 at epoch 365\n",
      "Epoch 371, Loss: 0.018869 *** Best loss so far: 0.018752 at epoch 370\n",
      "Epoch 372, Loss: 0.018837 *** Best loss so far: 0.018752 at epoch 370\n",
      "Epoch 373, Loss: 0.018707 *** Best loss so far: 0.018752 at epoch 370\n",
      "Epoch 374, Loss: 0.018773 *** Best loss so far: 0.018707 at epoch 373\n",
      "Epoch 375, Loss: 0.018780 *** Best loss so far: 0.018707 at epoch 373\n",
      "Epoch 376, Loss: 0.018718 *** Best loss so far: 0.018707 at epoch 373\n",
      "Epoch 377, Loss: 0.018797 *** Best loss so far: 0.018707 at epoch 373\n",
      "Epoch 378, Loss: 0.018723 *** Best loss so far: 0.018707 at epoch 373\n",
      "Epoch 379, Loss: 0.018764 *** Best loss so far: 0.018707 at epoch 373\n",
      "Epoch 380, Loss: 0.018735 *** Best loss so far: 0.018707 at epoch 373\n",
      "Epoch 381, Loss: 0.018772 *** Best loss so far: 0.018707 at epoch 373\n",
      "Epoch 382, Loss: 0.018777 *** Best loss so far: 0.018707 at epoch 373\n",
      "Epoch 383, Loss: 0.018754 *** Best loss so far: 0.018707 at epoch 373\n",
      "Epoch 384, Loss: 0.018637 *** Best loss so far: 0.018707 at epoch 373\n",
      "Epoch 385, Loss: 0.018792 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 386, Loss: 0.018751 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 387, Loss: 0.018810 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 388, Loss: 0.018695 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 389, Loss: 0.018730 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 390, Loss: 0.018778 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 391, Loss: 0.018750 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 392, Loss: 0.018644 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 393, Loss: 0.018813 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 394, Loss: 0.018707 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 395, Loss: 0.018683 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 396, Loss: 0.018674 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 397, Loss: 0.018697 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 398, Loss: 0.018759 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 399, Loss: 0.018717 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 400, Loss: 0.018728 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 401, Loss: 0.018705 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 402, Loss: 0.018734 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 403, Loss: 0.018658 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 404, Loss: 0.018714 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 405, Loss: 0.018631 *** Best loss so far: 0.018637 at epoch 384\n",
      "Epoch 406, Loss: 0.018693 *** Best loss so far: 0.018631 at epoch 405\n",
      "Epoch 407, Loss: 0.018599 *** Best loss so far: 0.018631 at epoch 405\n",
      "Epoch 408, Loss: 0.018675 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 409, Loss: 0.018684 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 410, Loss: 0.018673 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 411, Loss: 0.018730 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 412, Loss: 0.018666 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 413, Loss: 0.018660 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 414, Loss: 0.018652 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 415, Loss: 0.018724 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 416, Loss: 0.018685 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 417, Loss: 0.018631 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 418, Loss: 0.018677 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 419, Loss: 0.018626 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 420, Loss: 0.018680 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 421, Loss: 0.018586 *** Best loss so far: 0.018599 at epoch 407\n",
      "Epoch 422, Loss: 0.018620 *** Best loss so far: 0.018586 at epoch 421\n",
      "Epoch 423, Loss: 0.018582 *** Best loss so far: 0.018586 at epoch 421\n",
      "Epoch 424, Loss: 0.018589 *** Best loss so far: 0.018582 at epoch 423\n",
      "Epoch 425, Loss: 0.018654 *** Best loss so far: 0.018582 at epoch 423\n",
      "Epoch 426, Loss: 0.018629 *** Best loss so far: 0.018582 at epoch 423\n",
      "Epoch 427, Loss: 0.018658 *** Best loss so far: 0.018582 at epoch 423\n",
      "Epoch 428, Loss: 0.018668 *** Best loss so far: 0.018582 at epoch 423\n",
      "Epoch 429, Loss: 0.018601 *** Best loss so far: 0.018582 at epoch 423\n",
      "Epoch 430, Loss: 0.018565 *** Best loss so far: 0.018582 at epoch 423\n",
      "Epoch 431, Loss: 0.018588 *** Best loss so far: 0.018565 at epoch 430\n",
      "Epoch 432, Loss: 0.018634 *** Best loss so far: 0.018565 at epoch 430\n",
      "Epoch 433, Loss: 0.018609 *** Best loss so far: 0.018565 at epoch 430\n",
      "Epoch 434, Loss: 0.018533 *** Best loss so far: 0.018565 at epoch 430\n",
      "Epoch 435, Loss: 0.018711 *** Best loss so far: 0.018533 at epoch 434\n",
      "Epoch 436, Loss: 0.018566 *** Best loss so far: 0.018533 at epoch 434\n",
      "Epoch 437, Loss: 0.018652 *** Best loss so far: 0.018533 at epoch 434\n",
      "Epoch 438, Loss: 0.018636 *** Best loss so far: 0.018533 at epoch 434\n",
      "Epoch 439, Loss: 0.018589 *** Best loss so far: 0.018533 at epoch 434\n",
      "Epoch 440, Loss: 0.018718 *** Best loss so far: 0.018533 at epoch 434\n",
      "Epoch 441, Loss: 0.018596 *** Best loss so far: 0.018533 at epoch 434\n",
      "Epoch 442, Loss: 0.018593 *** Best loss so far: 0.018533 at epoch 434\n",
      "Epoch 443, Loss: 0.018603 *** Best loss so far: 0.018533 at epoch 434\n",
      "Epoch 444, Loss: 0.018599 *** Best loss so far: 0.018533 at epoch 434\n",
      "Epoch 445, Loss: 0.018576 *** Best loss so far: 0.018533 at epoch 434\n",
      "Epoch 446, Loss: 0.018529 *** Best loss so far: 0.018533 at epoch 434\n",
      "Epoch 447, Loss: 0.018600 *** Best loss so far: 0.018529 at epoch 446\n",
      "Epoch 448, Loss: 0.018605 *** Best loss so far: 0.018529 at epoch 446\n",
      "Epoch 449, Loss: 0.018480 *** Best loss so far: 0.018529 at epoch 446\n",
      "Epoch 450, Loss: 0.018600 *** Best loss so far: 0.018480 at epoch 449\n",
      "Epoch 451, Loss: 0.018482 *** Best loss so far: 0.018480 at epoch 449\n",
      "Epoch 452, Loss: 0.018597 *** Best loss so far: 0.018480 at epoch 449\n",
      "Epoch 453, Loss: 0.018511 *** Best loss so far: 0.018480 at epoch 449\n",
      "Epoch 454, Loss: 0.018536 *** Best loss so far: 0.018480 at epoch 449\n",
      "Epoch 455, Loss: 0.018588 *** Best loss so far: 0.018480 at epoch 449\n",
      "Epoch 456, Loss: 0.018604 *** Best loss so far: 0.018480 at epoch 449\n",
      "Epoch 457, Loss: 0.018514 *** Best loss so far: 0.018480 at epoch 449\n",
      "Epoch 458, Loss: 0.018546 *** Best loss so far: 0.018480 at epoch 449\n",
      "Epoch 459, Loss: 0.018546 *** Best loss so far: 0.018480 at epoch 449\n",
      "Epoch 460, Loss: 0.018631 *** Best loss so far: 0.018480 at epoch 449\n",
      "Epoch 461, Loss: 0.018475 *** Best loss so far: 0.018480 at epoch 449\n",
      "Epoch 462, Loss: 0.018504 *** Best loss so far: 0.018475 at epoch 461\n",
      "Epoch 463, Loss: 0.018512 *** Best loss so far: 0.018475 at epoch 461\n",
      "Epoch 464, Loss: 0.018560 *** Best loss so far: 0.018475 at epoch 461\n",
      "Epoch 465, Loss: 0.018467 *** Best loss so far: 0.018475 at epoch 461\n",
      "Epoch 466, Loss: 0.018574 *** Best loss so far: 0.018467 at epoch 465\n",
      "Epoch 467, Loss: 0.018513 *** Best loss so far: 0.018467 at epoch 465\n",
      "Epoch 468, Loss: 0.018470 *** Best loss so far: 0.018467 at epoch 465\n",
      "Epoch 469, Loss: 0.018476 *** Best loss so far: 0.018467 at epoch 465\n",
      "Epoch 470, Loss: 0.018428 *** Best loss so far: 0.018467 at epoch 465\n",
      "Epoch 471, Loss: 0.018459 *** Best loss so far: 0.018428 at epoch 470\n",
      "Epoch 472, Loss: 0.018532 *** Best loss so far: 0.018428 at epoch 470\n",
      "Epoch 473, Loss: 0.018517 *** Best loss so far: 0.018428 at epoch 470\n",
      "Epoch 474, Loss: 0.018518 *** Best loss so far: 0.018428 at epoch 470\n",
      "Epoch 475, Loss: 0.018452 *** Best loss so far: 0.018428 at epoch 470\n",
      "Epoch 476, Loss: 0.018494 *** Best loss so far: 0.018428 at epoch 470\n",
      "Epoch 477, Loss: 0.018426 *** Best loss so far: 0.018428 at epoch 470\n",
      "Epoch 478, Loss: 0.018464 *** Best loss so far: 0.018426 at epoch 477\n",
      "Epoch 479, Loss: 0.018473 *** Best loss so far: 0.018426 at epoch 477\n",
      "Epoch 480, Loss: 0.018517 *** Best loss so far: 0.018426 at epoch 477\n",
      "Epoch 481, Loss: 0.018443 *** Best loss so far: 0.018426 at epoch 477\n",
      "Epoch 482, Loss: 0.018499 *** Best loss so far: 0.018426 at epoch 477\n",
      "Epoch 483, Loss: 0.018466 *** Best loss so far: 0.018426 at epoch 477\n",
      "Epoch 484, Loss: 0.018476 *** Best loss so far: 0.018426 at epoch 477\n",
      "Epoch 485, Loss: 0.018489 *** Best loss so far: 0.018426 at epoch 477\n",
      "Epoch 486, Loss: 0.018362 *** Best loss so far: 0.018426 at epoch 477\n",
      "Epoch 487, Loss: 0.018443 *** Best loss so far: 0.018362 at epoch 486\n",
      "Epoch 488, Loss: 0.018467 *** Best loss so far: 0.018362 at epoch 486\n",
      "Epoch 489, Loss: 0.018500 *** Best loss so far: 0.018362 at epoch 486\n",
      "Epoch 490, Loss: 0.018454 *** Best loss so far: 0.018362 at epoch 486\n",
      "Epoch 491, Loss: 0.018392 *** Best loss so far: 0.018362 at epoch 486\n",
      "Epoch 492, Loss: 0.018464 *** Best loss so far: 0.018362 at epoch 486\n",
      "Epoch 493, Loss: 0.018422 *** Best loss so far: 0.018362 at epoch 486\n",
      "Epoch 494, Loss: 0.018394 *** Best loss so far: 0.018362 at epoch 486\n",
      "Epoch 495, Loss: 0.018439 *** Best loss so far: 0.018362 at epoch 486\n",
      "Epoch 496, Loss: 0.018341 *** Best loss so far: 0.018362 at epoch 486\n",
      "Epoch 497, Loss: 0.018414 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 498, Loss: 0.018477 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 499, Loss: 0.018432 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 500, Loss: 0.018380 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 501, Loss: 0.018371 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 502, Loss: 0.018373 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 503, Loss: 0.018395 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 504, Loss: 0.018410 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 505, Loss: 0.018395 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 506, Loss: 0.018352 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 507, Loss: 0.018434 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 508, Loss: 0.018376 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 509, Loss: 0.018373 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 510, Loss: 0.018396 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 511, Loss: 0.018319 *** Best loss so far: 0.018341 at epoch 496\n",
      "Epoch 512, Loss: 0.018418 *** Best loss so far: 0.018319 at epoch 511\n",
      "Epoch 513, Loss: 0.018350 *** Best loss so far: 0.018319 at epoch 511\n",
      "Epoch 514, Loss: 0.018333 *** Best loss so far: 0.018319 at epoch 511\n",
      "Epoch 515, Loss: 0.018399 *** Best loss so far: 0.018319 at epoch 511\n",
      "Epoch 516, Loss: 0.018354 *** Best loss so far: 0.018319 at epoch 511\n",
      "Epoch 517, Loss: 0.018411 *** Best loss so far: 0.018319 at epoch 511\n",
      "Epoch 518, Loss: 0.018358 *** Best loss so far: 0.018319 at epoch 511\n",
      "Epoch 519, Loss: 0.018369 *** Best loss so far: 0.018319 at epoch 511\n",
      "Epoch 520, Loss: 0.018386 *** Best loss so far: 0.018319 at epoch 511\n",
      "Epoch 521, Loss: 0.018328 *** Best loss so far: 0.018319 at epoch 511\n",
      "Epoch 522, Loss: 0.018345 *** Best loss so far: 0.018319 at epoch 511\n",
      "Epoch 523, Loss: 0.018292 *** Best loss so far: 0.018319 at epoch 511\n",
      "Epoch 524, Loss: 0.018371 *** Best loss so far: 0.018292 at epoch 523\n",
      "Epoch 525, Loss: 0.018295 *** Best loss so far: 0.018292 at epoch 523\n",
      "Epoch 526, Loss: 0.018312 *** Best loss so far: 0.018292 at epoch 523\n",
      "Epoch 527, Loss: 0.018360 *** Best loss so far: 0.018292 at epoch 523\n",
      "Epoch 528, Loss: 0.018292 *** Best loss so far: 0.018292 at epoch 523\n",
      "Epoch 529, Loss: 0.018335 *** Best loss so far: 0.018292 at epoch 523\n",
      "Epoch 530, Loss: 0.018313 *** Best loss so far: 0.018292 at epoch 523\n",
      "Epoch 531, Loss: 0.018335 *** Best loss so far: 0.018292 at epoch 523\n",
      "Epoch 532, Loss: 0.018303 *** Best loss so far: 0.018292 at epoch 523\n",
      "Epoch 533, Loss: 0.018291 *** Best loss so far: 0.018292 at epoch 523\n",
      "Epoch 534, Loss: 0.018306 *** Best loss so far: 0.018291 at epoch 533\n",
      "Epoch 535, Loss: 0.018316 *** Best loss so far: 0.018291 at epoch 533\n",
      "Epoch 536, Loss: 0.018302 *** Best loss so far: 0.018291 at epoch 533\n",
      "Epoch 537, Loss: 0.018345 *** Best loss so far: 0.018291 at epoch 533\n",
      "Epoch 538, Loss: 0.018230 *** Best loss so far: 0.018291 at epoch 533\n",
      "Epoch 539, Loss: 0.018222 *** Best loss so far: 0.018230 at epoch 538\n",
      "Epoch 540, Loss: 0.018366 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 541, Loss: 0.018315 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 542, Loss: 0.018288 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 543, Loss: 0.018371 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 544, Loss: 0.018258 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 545, Loss: 0.018261 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 546, Loss: 0.018285 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 547, Loss: 0.018282 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 548, Loss: 0.018272 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 549, Loss: 0.018261 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 550, Loss: 0.018232 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 551, Loss: 0.018293 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 552, Loss: 0.018287 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 553, Loss: 0.018259 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 554, Loss: 0.018321 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 555, Loss: 0.018267 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 556, Loss: 0.018246 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 557, Loss: 0.018247 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 558, Loss: 0.018251 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 559, Loss: 0.018202 *** Best loss so far: 0.018222 at epoch 539\n",
      "Epoch 560, Loss: 0.018246 *** Best loss so far: 0.018202 at epoch 559\n",
      "Epoch 561, Loss: 0.018227 *** Best loss so far: 0.018202 at epoch 559\n",
      "Epoch 562, Loss: 0.018283 *** Best loss so far: 0.018202 at epoch 559\n",
      "Epoch 563, Loss: 0.018255 *** Best loss so far: 0.018202 at epoch 559\n",
      "Epoch 564, Loss: 0.018265 *** Best loss so far: 0.018202 at epoch 559\n",
      "Epoch 565, Loss: 0.018239 *** Best loss so far: 0.018202 at epoch 559\n",
      "Epoch 566, Loss: 0.018249 *** Best loss so far: 0.018202 at epoch 559\n",
      "Epoch 567, Loss: 0.018179 *** Best loss so far: 0.018202 at epoch 559\n",
      "Epoch 568, Loss: 0.018253 *** Best loss so far: 0.018179 at epoch 567\n",
      "Epoch 569, Loss: 0.018297 *** Best loss so far: 0.018179 at epoch 567\n",
      "Epoch 570, Loss: 0.018237 *** Best loss so far: 0.018179 at epoch 567\n",
      "Epoch 571, Loss: 0.018268 *** Best loss so far: 0.018179 at epoch 567\n",
      "Epoch 572, Loss: 0.018202 *** Best loss so far: 0.018179 at epoch 567\n",
      "Epoch 573, Loss: 0.018214 *** Best loss so far: 0.018179 at epoch 567\n",
      "Epoch 574, Loss: 0.018229 *** Best loss so far: 0.018179 at epoch 567\n",
      "Epoch 575, Loss: 0.018174 *** Best loss so far: 0.018179 at epoch 567\n",
      "Epoch 576, Loss: 0.018214 *** Best loss so far: 0.018174 at epoch 575\n",
      "Epoch 577, Loss: 0.018300 *** Best loss so far: 0.018174 at epoch 575\n",
      "Epoch 578, Loss: 0.018195 *** Best loss so far: 0.018174 at epoch 575\n",
      "Epoch 579, Loss: 0.018257 *** Best loss so far: 0.018174 at epoch 575\n",
      "Epoch 580, Loss: 0.018209 *** Best loss so far: 0.018174 at epoch 575\n",
      "Epoch 581, Loss: 0.018214 *** Best loss so far: 0.018174 at epoch 575\n",
      "Epoch 582, Loss: 0.018141 *** Best loss so far: 0.018174 at epoch 575\n",
      "Epoch 583, Loss: 0.018210 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 584, Loss: 0.018228 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 585, Loss: 0.018211 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 586, Loss: 0.018214 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 587, Loss: 0.018200 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 588, Loss: 0.018189 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 589, Loss: 0.018246 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 590, Loss: 0.018152 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 591, Loss: 0.018174 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 592, Loss: 0.018216 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 593, Loss: 0.018242 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 594, Loss: 0.018143 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 595, Loss: 0.018218 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 596, Loss: 0.018169 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 597, Loss: 0.018263 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 598, Loss: 0.018133 *** Best loss so far: 0.018141 at epoch 582\n",
      "Epoch 599, Loss: 0.018200 *** Best loss so far: 0.018133 at epoch 598\n",
      "Epoch 600, Loss: 0.018172 *** Best loss so far: 0.018133 at epoch 598\n",
      "Epoch 601, Loss: 0.018234 *** Best loss so far: 0.018133 at epoch 598\n",
      "Epoch 602, Loss: 0.018204 *** Best loss so far: 0.018133 at epoch 598\n",
      "Epoch 603, Loss: 0.018189 *** Best loss so far: 0.018133 at epoch 598\n",
      "Epoch 604, Loss: 0.018178 *** Best loss so far: 0.018133 at epoch 598\n",
      "Epoch 605, Loss: 0.018140 *** Best loss so far: 0.018133 at epoch 598\n",
      "Epoch 606, Loss: 0.018197 *** Best loss so far: 0.018133 at epoch 598\n",
      "Epoch 607, Loss: 0.018188 *** Best loss so far: 0.018133 at epoch 598\n",
      "Epoch 608, Loss: 0.018123 *** Best loss so far: 0.018133 at epoch 598\n",
      "Epoch 609, Loss: 0.018176 *** Best loss so far: 0.018123 at epoch 608\n",
      "Epoch 610, Loss: 0.018178 *** Best loss so far: 0.018123 at epoch 608\n",
      "Epoch 611, Loss: 0.018175 *** Best loss so far: 0.018123 at epoch 608\n",
      "Epoch 612, Loss: 0.018230 *** Best loss so far: 0.018123 at epoch 608\n",
      "Epoch 613, Loss: 0.018156 *** Best loss so far: 0.018123 at epoch 608\n",
      "Epoch 614, Loss: 0.018173 *** Best loss so far: 0.018123 at epoch 608\n",
      "Epoch 615, Loss: 0.018156 *** Best loss so far: 0.018123 at epoch 608\n",
      "Epoch 616, Loss: 0.018219 *** Best loss so far: 0.018123 at epoch 608\n",
      "Epoch 617, Loss: 0.018156 *** Best loss so far: 0.018123 at epoch 608\n",
      "Epoch 618, Loss: 0.018105 *** Best loss so far: 0.018123 at epoch 608\n",
      "Epoch 619, Loss: 0.018199 *** Best loss so far: 0.018105 at epoch 618\n",
      "Epoch 620, Loss: 0.018105 *** Best loss so far: 0.018105 at epoch 618\n",
      "Epoch 621, Loss: 0.018136 *** Best loss so far: 0.018105 at epoch 620\n",
      "Epoch 622, Loss: 0.018122 *** Best loss so far: 0.018105 at epoch 620\n",
      "Epoch 623, Loss: 0.018139 *** Best loss so far: 0.018105 at epoch 620\n",
      "Epoch 624, Loss: 0.018124 *** Best loss so far: 0.018105 at epoch 620\n",
      "Epoch 625, Loss: 0.018147 *** Best loss so far: 0.018105 at epoch 620\n",
      "Epoch 626, Loss: 0.018146 *** Best loss so far: 0.018105 at epoch 620\n",
      "Epoch 627, Loss: 0.018127 *** Best loss so far: 0.018105 at epoch 620\n",
      "Epoch 628, Loss: 0.018157 *** Best loss so far: 0.018105 at epoch 620\n",
      "Epoch 629, Loss: 0.018107 *** Best loss so far: 0.018105 at epoch 620\n",
      "Epoch 630, Loss: 0.018149 *** Best loss so far: 0.018105 at epoch 620\n",
      "Epoch 631, Loss: 0.018130 *** Best loss so far: 0.018105 at epoch 620\n",
      "Epoch 632, Loss: 0.018133 *** Best loss so far: 0.018105 at epoch 620\n",
      "Epoch 633, Loss: 0.018050 *** Best loss so far: 0.018105 at epoch 620\n",
      "Epoch 634, Loss: 0.018135 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 635, Loss: 0.018117 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 636, Loss: 0.018115 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 637, Loss: 0.018127 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 638, Loss: 0.018127 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 639, Loss: 0.018071 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 640, Loss: 0.018097 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 641, Loss: 0.018135 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 642, Loss: 0.018136 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 643, Loss: 0.018123 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 644, Loss: 0.018075 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 645, Loss: 0.018113 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 646, Loss: 0.018079 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 647, Loss: 0.018098 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 648, Loss: 0.018088 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 649, Loss: 0.018135 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 650, Loss: 0.018150 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 651, Loss: 0.018123 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 652, Loss: 0.018087 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 653, Loss: 0.018098 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 654, Loss: 0.018086 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 655, Loss: 0.018104 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 656, Loss: 0.018054 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 657, Loss: 0.018165 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 658, Loss: 0.018064 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 659, Loss: 0.018160 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 660, Loss: 0.018105 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 661, Loss: 0.018093 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 662, Loss: 0.018037 *** Best loss so far: 0.018050 at epoch 633\n",
      "Epoch 663, Loss: 0.018102 *** Best loss so far: 0.018037 at epoch 662\n",
      "Epoch 664, Loss: 0.018127 *** Best loss so far: 0.018037 at epoch 662\n",
      "Epoch 665, Loss: 0.018103 *** Best loss so far: 0.018037 at epoch 662\n",
      "Epoch 666, Loss: 0.018074 *** Best loss so far: 0.018037 at epoch 662\n",
      "Epoch 667, Loss: 0.018061 *** Best loss so far: 0.018037 at epoch 662\n",
      "Epoch 668, Loss: 0.018027 *** Best loss so far: 0.018037 at epoch 662\n",
      "Epoch 669, Loss: 0.018105 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 670, Loss: 0.018071 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 671, Loss: 0.018091 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 672, Loss: 0.018050 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 673, Loss: 0.018118 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 674, Loss: 0.018075 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 675, Loss: 0.018065 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 676, Loss: 0.018078 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 677, Loss: 0.018110 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 678, Loss: 0.018065 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 679, Loss: 0.018079 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 680, Loss: 0.018092 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 681, Loss: 0.018044 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 682, Loss: 0.018043 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 683, Loss: 0.018037 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 684, Loss: 0.018101 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 685, Loss: 0.018068 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 686, Loss: 0.018066 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 687, Loss: 0.018036 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 688, Loss: 0.018075 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 689, Loss: 0.018040 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 690, Loss: 0.018062 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 691, Loss: 0.018054 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 692, Loss: 0.018090 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 693, Loss: 0.018071 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 694, Loss: 0.018095 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 695, Loss: 0.018079 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 696, Loss: 0.018063 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 697, Loss: 0.018032 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 698, Loss: 0.018061 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 699, Loss: 0.018064 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 700, Loss: 0.018061 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 701, Loss: 0.018009 *** Best loss so far: 0.018027 at epoch 668\n",
      "Epoch 702, Loss: 0.018034 *** Best loss so far: 0.018009 at epoch 701\n",
      "Epoch 703, Loss: 0.018050 *** Best loss so far: 0.018009 at epoch 701\n",
      "Epoch 704, Loss: 0.018029 *** Best loss so far: 0.018009 at epoch 701\n",
      "Epoch 705, Loss: 0.018040 *** Best loss so far: 0.018009 at epoch 701\n",
      "Epoch 706, Loss: 0.018008 *** Best loss so far: 0.018009 at epoch 701\n",
      "Epoch 707, Loss: 0.018046 *** Best loss so far: 0.018008 at epoch 706\n",
      "Epoch 708, Loss: 0.018042 *** Best loss so far: 0.018008 at epoch 706\n",
      "Epoch 709, Loss: 0.018018 *** Best loss so far: 0.018008 at epoch 706\n",
      "Epoch 710, Loss: 0.018050 *** Best loss so far: 0.018008 at epoch 706\n",
      "Epoch 711, Loss: 0.018001 *** Best loss so far: 0.018008 at epoch 706\n",
      "Epoch 712, Loss: 0.018036 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 713, Loss: 0.018052 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 714, Loss: 0.018019 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 715, Loss: 0.018034 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 716, Loss: 0.018023 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 717, Loss: 0.018053 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 718, Loss: 0.018004 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 719, Loss: 0.018026 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 720, Loss: 0.018023 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 721, Loss: 0.018038 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 722, Loss: 0.018054 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 723, Loss: 0.018035 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 724, Loss: 0.018024 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 725, Loss: 0.018028 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 726, Loss: 0.017978 *** Best loss so far: 0.018001 at epoch 711\n",
      "Epoch 727, Loss: 0.018031 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 728, Loss: 0.017995 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 729, Loss: 0.017992 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 730, Loss: 0.018050 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 731, Loss: 0.018050 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 732, Loss: 0.018033 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 733, Loss: 0.018001 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 734, Loss: 0.018014 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 735, Loss: 0.018003 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 736, Loss: 0.018021 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 737, Loss: 0.018016 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 738, Loss: 0.018046 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 739, Loss: 0.018028 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 740, Loss: 0.018041 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 741, Loss: 0.018026 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 742, Loss: 0.017984 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 743, Loss: 0.018003 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 744, Loss: 0.018035 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 745, Loss: 0.018005 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 746, Loss: 0.018019 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 747, Loss: 0.017993 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 748, Loss: 0.018004 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 749, Loss: 0.018064 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 750, Loss: 0.017970 *** Best loss so far: 0.017978 at epoch 726\n",
      "Epoch 751, Loss: 0.018038 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 752, Loss: 0.017978 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 753, Loss: 0.018018 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 754, Loss: 0.017998 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 755, Loss: 0.018009 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 756, Loss: 0.018010 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 757, Loss: 0.017977 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 758, Loss: 0.018025 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 759, Loss: 0.017982 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 760, Loss: 0.018007 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 761, Loss: 0.017998 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 762, Loss: 0.017999 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 763, Loss: 0.018008 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 764, Loss: 0.018007 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 765, Loss: 0.017997 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 766, Loss: 0.018001 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 767, Loss: 0.017998 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 768, Loss: 0.017966 *** Best loss so far: 0.017970 at epoch 750\n",
      "Epoch 769, Loss: 0.018016 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 770, Loss: 0.017969 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 771, Loss: 0.017995 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 772, Loss: 0.018005 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 773, Loss: 0.018001 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 774, Loss: 0.017987 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 775, Loss: 0.018006 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 776, Loss: 0.017975 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 777, Loss: 0.017977 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 778, Loss: 0.017975 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 779, Loss: 0.017984 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 780, Loss: 0.017976 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 781, Loss: 0.017990 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 782, Loss: 0.017999 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 783, Loss: 0.017997 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 784, Loss: 0.017988 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 785, Loss: 0.017995 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 786, Loss: 0.017972 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 787, Loss: 0.017933 *** Best loss so far: 0.017966 at epoch 768\n",
      "Epoch 788, Loss: 0.017983 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 789, Loss: 0.017976 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 790, Loss: 0.017989 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 791, Loss: 0.018008 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 792, Loss: 0.017990 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 793, Loss: 0.017983 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 794, Loss: 0.017989 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 795, Loss: 0.017971 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 796, Loss: 0.017947 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 797, Loss: 0.017957 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 798, Loss: 0.017981 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 799, Loss: 0.017968 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 800, Loss: 0.018009 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 801, Loss: 0.017967 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 802, Loss: 0.017968 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 803, Loss: 0.017979 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 804, Loss: 0.017977 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 805, Loss: 0.017967 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 806, Loss: 0.017956 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 807, Loss: 0.017956 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 808, Loss: 0.017956 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 809, Loss: 0.018023 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 810, Loss: 0.017956 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 811, Loss: 0.017976 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 812, Loss: 0.017974 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 813, Loss: 0.017960 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 814, Loss: 0.017954 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 815, Loss: 0.017973 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 816, Loss: 0.017930 *** Best loss so far: 0.017933 at epoch 787\n",
      "Epoch 817, Loss: 0.017956 *** Best loss so far: 0.017930 at epoch 816\n",
      "Epoch 818, Loss: 0.017988 *** Best loss so far: 0.017930 at epoch 816\n",
      "Epoch 819, Loss: 0.017988 *** Best loss so far: 0.017930 at epoch 816\n",
      "Epoch 820, Loss: 0.017937 *** Best loss so far: 0.017930 at epoch 816\n",
      "Epoch 821, Loss: 0.017979 *** Best loss so far: 0.017930 at epoch 816\n",
      "Epoch 822, Loss: 0.017927 *** Best loss so far: 0.017930 at epoch 816\n",
      "Epoch 823, Loss: 0.017983 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 824, Loss: 0.017970 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 825, Loss: 0.017967 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 826, Loss: 0.017976 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 827, Loss: 0.017961 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 828, Loss: 0.017979 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 829, Loss: 0.017959 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 830, Loss: 0.017977 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 831, Loss: 0.017951 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 832, Loss: 0.017974 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 833, Loss: 0.017949 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 834, Loss: 0.017975 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 835, Loss: 0.017980 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 836, Loss: 0.017968 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 837, Loss: 0.017948 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 838, Loss: 0.017946 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 839, Loss: 0.017941 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 840, Loss: 0.017986 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 841, Loss: 0.017974 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 842, Loss: 0.017952 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 843, Loss: 0.017984 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 844, Loss: 0.017966 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 845, Loss: 0.017953 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 846, Loss: 0.017928 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 847, Loss: 0.017956 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 848, Loss: 0.017932 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 849, Loss: 0.017967 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 850, Loss: 0.017947 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 851, Loss: 0.017989 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 852, Loss: 0.017959 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 853, Loss: 0.017967 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 854, Loss: 0.017974 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 855, Loss: 0.017951 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 856, Loss: 0.017956 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 857, Loss: 0.017964 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 858, Loss: 0.017947 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 859, Loss: 0.017949 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 860, Loss: 0.017966 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 861, Loss: 0.017958 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 862, Loss: 0.017953 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 863, Loss: 0.017954 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 864, Loss: 0.017935 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 865, Loss: 0.017964 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 866, Loss: 0.017934 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 867, Loss: 0.017967 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 868, Loss: 0.017976 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 869, Loss: 0.017944 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 870, Loss: 0.017926 *** Best loss so far: 0.017927 at epoch 822\n",
      "Epoch 871, Loss: 0.017946 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 872, Loss: 0.017937 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 873, Loss: 0.017941 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 874, Loss: 0.017957 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 875, Loss: 0.017964 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 876, Loss: 0.017946 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 877, Loss: 0.017949 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 878, Loss: 0.017948 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 879, Loss: 0.017953 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 880, Loss: 0.017957 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 881, Loss: 0.017936 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 882, Loss: 0.017962 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 883, Loss: 0.017939 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 884, Loss: 0.017948 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 885, Loss: 0.017929 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 886, Loss: 0.017937 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 887, Loss: 0.017954 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 888, Loss: 0.017930 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 889, Loss: 0.017939 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 890, Loss: 0.017936 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 891, Loss: 0.017941 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 892, Loss: 0.017951 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 893, Loss: 0.017942 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 894, Loss: 0.017937 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 895, Loss: 0.017957 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 896, Loss: 0.017938 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 897, Loss: 0.017949 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 898, Loss: 0.017938 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 899, Loss: 0.017928 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 900, Loss: 0.017952 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 901, Loss: 0.017925 *** Best loss so far: 0.017926 at epoch 870\n",
      "Epoch 902, Loss: 0.017950 *** Best loss so far: 0.017925 at epoch 901\n",
      "Epoch 903, Loss: 0.017901 *** Best loss so far: 0.017925 at epoch 901\n",
      "Epoch 904, Loss: 0.017942 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 905, Loss: 0.017953 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 906, Loss: 0.017942 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 907, Loss: 0.017937 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 908, Loss: 0.017930 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 909, Loss: 0.017964 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 910, Loss: 0.017950 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 911, Loss: 0.017933 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 912, Loss: 0.017919 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 913, Loss: 0.017953 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 914, Loss: 0.017937 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 915, Loss: 0.017930 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 916, Loss: 0.017930 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 917, Loss: 0.017924 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 918, Loss: 0.017929 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 919, Loss: 0.017946 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 920, Loss: 0.017923 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 921, Loss: 0.017954 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 922, Loss: 0.017934 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 923, Loss: 0.017911 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 924, Loss: 0.017933 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 925, Loss: 0.017955 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 926, Loss: 0.017912 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 927, Loss: 0.017946 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 928, Loss: 0.017930 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 929, Loss: 0.017931 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 930, Loss: 0.017922 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 931, Loss: 0.017951 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 932, Loss: 0.017917 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 933, Loss: 0.017940 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 934, Loss: 0.017938 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 935, Loss: 0.017936 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 936, Loss: 0.017946 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 937, Loss: 0.017943 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 938, Loss: 0.017922 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 939, Loss: 0.017931 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 940, Loss: 0.017941 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 941, Loss: 0.017926 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 942, Loss: 0.017922 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 943, Loss: 0.017940 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 944, Loss: 0.017929 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 945, Loss: 0.017941 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 946, Loss: 0.017928 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 947, Loss: 0.017932 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 948, Loss: 0.017903 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 949, Loss: 0.017945 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 950, Loss: 0.017932 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 951, Loss: 0.017928 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 952, Loss: 0.017922 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 953, Loss: 0.017939 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 954, Loss: 0.017920 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 955, Loss: 0.017929 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 956, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 957, Loss: 0.017929 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 958, Loss: 0.017942 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 959, Loss: 0.017929 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 960, Loss: 0.017930 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 961, Loss: 0.017925 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 962, Loss: 0.017921 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 963, Loss: 0.017927 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 964, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 965, Loss: 0.017946 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 966, Loss: 0.017920 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 967, Loss: 0.017936 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 968, Loss: 0.017919 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 969, Loss: 0.017926 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 970, Loss: 0.017931 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 971, Loss: 0.017926 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 972, Loss: 0.017919 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 973, Loss: 0.017935 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 974, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 975, Loss: 0.017941 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 976, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 977, Loss: 0.017924 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 978, Loss: 0.017917 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 979, Loss: 0.017927 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 980, Loss: 0.017930 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 981, Loss: 0.017933 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 982, Loss: 0.017908 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 983, Loss: 0.017917 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 984, Loss: 0.017924 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 985, Loss: 0.017938 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 986, Loss: 0.017930 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 987, Loss: 0.017911 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 988, Loss: 0.017929 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 989, Loss: 0.017908 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 990, Loss: 0.017929 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 991, Loss: 0.017917 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 992, Loss: 0.017920 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 993, Loss: 0.017926 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 994, Loss: 0.017914 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 995, Loss: 0.017910 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 996, Loss: 0.017931 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 997, Loss: 0.017903 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 998, Loss: 0.017922 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 999, Loss: 0.017925 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1000, Loss: 0.017919 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1001, Loss: 0.017937 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1002, Loss: 0.017923 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1003, Loss: 0.017911 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1004, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1005, Loss: 0.017911 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1006, Loss: 0.017920 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1007, Loss: 0.017937 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1008, Loss: 0.017903 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1009, Loss: 0.017912 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1010, Loss: 0.017928 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1011, Loss: 0.017921 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1012, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1013, Loss: 0.017927 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1014, Loss: 0.017926 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1015, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1016, Loss: 0.017929 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1017, Loss: 0.017945 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1018, Loss: 0.017917 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1019, Loss: 0.017924 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1020, Loss: 0.017932 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1021, Loss: 0.017933 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1022, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1023, Loss: 0.017923 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1024, Loss: 0.017930 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1025, Loss: 0.017920 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1026, Loss: 0.017912 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1027, Loss: 0.017931 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1028, Loss: 0.017914 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1029, Loss: 0.017913 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1030, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1031, Loss: 0.017930 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1032, Loss: 0.017912 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1033, Loss: 0.017927 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1034, Loss: 0.017917 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1035, Loss: 0.017927 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1036, Loss: 0.017925 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1037, Loss: 0.017923 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1038, Loss: 0.017932 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1039, Loss: 0.017922 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1040, Loss: 0.017930 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1041, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1042, Loss: 0.017927 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1043, Loss: 0.017929 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1044, Loss: 0.017919 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1045, Loss: 0.017917 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1046, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1047, Loss: 0.017924 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1048, Loss: 0.017920 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1049, Loss: 0.017910 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1050, Loss: 0.017922 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1051, Loss: 0.017923 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1052, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1053, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1054, Loss: 0.017917 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1055, Loss: 0.017917 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1056, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1057, Loss: 0.017914 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1058, Loss: 0.017917 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1059, Loss: 0.017921 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1060, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1061, Loss: 0.017907 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1062, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1063, Loss: 0.017923 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1064, Loss: 0.017913 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1065, Loss: 0.017924 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1066, Loss: 0.017921 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1067, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1068, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1069, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1070, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1071, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1072, Loss: 0.017934 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1073, Loss: 0.017911 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1074, Loss: 0.017919 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1075, Loss: 0.017911 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1076, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1077, Loss: 0.017911 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1078, Loss: 0.017929 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1079, Loss: 0.017924 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1080, Loss: 0.017914 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1081, Loss: 0.017913 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1082, Loss: 0.017922 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1083, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1084, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1085, Loss: 0.017925 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1086, Loss: 0.017906 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1087, Loss: 0.017925 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1088, Loss: 0.017904 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1089, Loss: 0.017913 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1090, Loss: 0.017912 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1091, Loss: 0.017928 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1092, Loss: 0.017924 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1093, Loss: 0.017913 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1094, Loss: 0.017922 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1095, Loss: 0.017917 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1096, Loss: 0.017922 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1097, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1098, Loss: 0.017913 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1099, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1100, Loss: 0.017903 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1101, Loss: 0.017910 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1102, Loss: 0.017923 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1103, Loss: 0.017906 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1104, Loss: 0.017926 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1105, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1106, Loss: 0.017921 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1107, Loss: 0.017924 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1108, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1109, Loss: 0.017906 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1110, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1111, Loss: 0.017928 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1112, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1113, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1114, Loss: 0.017921 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1115, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1116, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1117, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1118, Loss: 0.017911 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1119, Loss: 0.017912 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1120, Loss: 0.017912 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1121, Loss: 0.017914 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1122, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1123, Loss: 0.017917 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1124, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1125, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1126, Loss: 0.017921 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1127, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1128, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1129, Loss: 0.017913 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1130, Loss: 0.017912 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1131, Loss: 0.017908 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1132, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1133, Loss: 0.017902 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1134, Loss: 0.017922 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1135, Loss: 0.017913 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1136, Loss: 0.017910 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1137, Loss: 0.017911 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1138, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1139, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1140, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1141, Loss: 0.017907 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1142, Loss: 0.017913 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1143, Loss: 0.017913 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1144, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1145, Loss: 0.017904 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1146, Loss: 0.017917 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1147, Loss: 0.017913 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1148, Loss: 0.017911 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1149, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1150, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1151, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1152, Loss: 0.017908 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1153, Loss: 0.017914 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1154, Loss: 0.017911 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1155, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1156, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1157, Loss: 0.017919 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1158, Loss: 0.017924 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1159, Loss: 0.017910 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1160, Loss: 0.017913 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1161, Loss: 0.017914 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1162, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1163, Loss: 0.017923 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1164, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1165, Loss: 0.017914 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1166, Loss: 0.017925 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1167, Loss: 0.017923 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1168, Loss: 0.017925 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1169, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1170, Loss: 0.017921 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1171, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1172, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1173, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1174, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1175, Loss: 0.017908 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1176, Loss: 0.017907 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1177, Loss: 0.017914 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1178, Loss: 0.017918 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1179, Loss: 0.017907 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1180, Loss: 0.017919 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1181, Loss: 0.017907 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1182, Loss: 0.017907 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1183, Loss: 0.017914 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1184, Loss: 0.017913 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1185, Loss: 0.017915 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1186, Loss: 0.017908 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1187, Loss: 0.017908 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1188, Loss: 0.017904 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1189, Loss: 0.017911 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1190, Loss: 0.017908 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1191, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1192, Loss: 0.017904 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1193, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1194, Loss: 0.017909 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1195, Loss: 0.017916 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1196, Loss: 0.017911 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1197, Loss: 0.017908 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1198, Loss: 0.017899 *** Best loss so far: 0.017901 at epoch 903\n",
      "Epoch 1199, Loss: 0.017919 *** Best loss so far: 0.017899 at epoch 1198\n"
     ]
    }
   ],
   "source": [
    "# edge_index = edge_index.to(device) \n",
    "best_models_dir = r\"D:\\Git_repos\\Photonic_GCN\"\n",
    "\n",
    "best_loss = 100\n",
    "best_loss_epoch = 1\n",
    "\n",
    "model.train()\n",
    "for epoch in range(1, EPOCHS):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader_train:\n",
    "        optimizer.zero_grad()\n",
    "        x, y = batch\n",
    "        x = x.float()\n",
    "        y = y.float() \n",
    "        # x.to(device); y.to(device) \n",
    "        out = model(x, edge_index) \n",
    "        y = y.view(out.shape)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() \n",
    "\n",
    "    scheduler.step()\n",
    "    avg_loss = total_loss / len(dataloader_train)  # Compute average loss for the epoch\n",
    "    print(f'Epoch {epoch}, Loss: {avg_loss:.6f} *** Best loss so far: {best_loss:.6f} at epoch {best_loss_epoch}')\n",
    "\n",
    "    if avg_loss < best_loss:\n",
    "        best_loss = avg_loss\n",
    "        best_loss_epoch = epoch\n",
    "        best_model_path = os.path.join(best_models_dir, \"best_model.pt\")\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        # print(f\"Best model saved with loss:  {avg_loss}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([415, 51, 3])\n",
      "torch.Size([415])\n"
     ]
    }
   ],
   "source": [
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 975.5\n",
      "real: 825.7\n",
      " -------------------------------- \n",
      "pred: 712.5\n",
      "real: 800.0\n",
      " -------------------------------- \n",
      "pred: 1060.7\n",
      "real: 1021.5\n",
      " -------------------------------- \n",
      "pred: 693.9\n",
      "real: 1363.1\n",
      " -------------------------------- \n",
      "pred: 1062.4\n",
      "real: 1185.0\n",
      " -------------------------------- \n",
      "pred: 832.9\n",
      "real: 1321.5\n",
      " -------------------------------- \n",
      "pred: 970.2\n",
      "real: 1200.9\n",
      " -------------------------------- \n",
      "pred: 675.5\n",
      "real: 1651.3\n",
      " -------------------------------- \n",
      "pred: 881.4\n",
      "real: 810.6\n",
      " -------------------------------- \n",
      "pred: 946.0\n",
      "real: 894.7\n",
      " -------------------------------- \n",
      "pred: 1461.5\n",
      "real: 1408.3\n",
      " -------------------------------- \n",
      "pred: 539.6\n",
      "real: 512.4\n",
      " -------------------------------- \n",
      "pred: 946.0\n",
      "real: 894.7\n",
      " -------------------------------- \n",
      "pred: 1209.9\n",
      "real: 1204.6\n",
      " -------------------------------- \n",
      "pred: 779.0\n",
      "real: 662.2\n",
      " -------------------------------- \n",
      "pred: 557.4\n",
      "real: 483.9\n",
      " -------------------------------- \n",
      "pred: 838.7\n",
      "real: 946.3\n",
      " -------------------------------- \n",
      "pred: 941.0\n",
      "real: 931.5\n",
      " -------------------------------- \n",
      "pred: 748.0\n",
      "real: 439.6\n",
      " -------------------------------- \n",
      "pred: 665.5\n",
      "real: 468.6\n",
      " -------------------------------- \n",
      "pred: 855.3\n",
      "real: 707.8\n",
      " -------------------------------- \n",
      "pred: 1091.5\n",
      "real: 879.2\n",
      " -------------------------------- \n",
      "pred: 747.7\n",
      "real: 562.3\n",
      " -------------------------------- \n",
      "pred: 933.3\n",
      "real: 1085.9\n",
      " -------------------------------- \n",
      "pred: 804.7\n",
      "real: 769.7\n",
      " -------------------------------- \n",
      "pred: 642.2\n",
      "real: 597.4\n",
      " -------------------------------- \n",
      "pred: 1133.0\n",
      "real: 720.8\n",
      " -------------------------------- \n",
      "pred: 862.7\n",
      "real: 945.3\n",
      " -------------------------------- \n",
      "pred: 895.7\n",
      "real: 1153.2\n",
      " -------------------------------- \n",
      "pred: 610.8\n",
      "real: 677.7\n",
      " -------------------------------- \n",
      "pred: 872.9\n",
      "real: 595.3\n",
      " -------------------------------- \n",
      "pred: 1065.0\n",
      "real: 1415.4\n",
      " -------------------------------- \n",
      "pred: 1570.2\n",
      "real: 2324.8\n",
      " -------------------------------- \n",
      "pred: 852.4\n",
      "real: 885.9\n",
      " -------------------------------- \n",
      "pred: 815.6\n",
      "real: 665.8\n",
      " -------------------------------- \n",
      "pred: 1433.6\n",
      "real: 1353.1\n",
      " -------------------------------- \n",
      "pred: 660.7\n",
      "real: 487.8\n",
      " -------------------------------- \n",
      "pred: 1059.0\n",
      "real: 1193.9\n",
      " -------------------------------- \n",
      "pred: 698.8\n",
      "real: 438.9\n",
      " -------------------------------- \n",
      "pred: 1018.5\n",
      "real: 644.6\n",
      " -------------------------------- \n",
      "pred: 2533.1\n",
      "real: 883.3\n",
      " -------------------------------- \n",
      "pred: 712.2\n",
      "real: 1316.4\n",
      " -------------------------------- \n",
      "pred: 666.3\n",
      "real: 503.4\n",
      " -------------------------------- \n",
      "pred: 927.8\n",
      "real: 551.5\n",
      " -------------------------------- \n",
      "pred: 877.4\n",
      "real: 823.6\n",
      " -------------------------------- \n",
      "pred: 1049.5\n",
      "real: 1300.9\n",
      " -------------------------------- \n",
      "pred: 1114.8\n",
      "real: 1244.4\n",
      " -------------------------------- \n",
      "pred: 1062.4\n",
      "real: 1185.0\n",
      " -------------------------------- \n",
      "pred: 680.0\n",
      "real: 461.1\n",
      " -------------------------------- \n",
      "pred: 694.8\n",
      "real: 1083.9\n",
      " -------------------------------- \n",
      "pred: 890.5\n",
      "real: 590.8\n",
      " -------------------------------- \n",
      "pred: 1091.9\n",
      "real: 1568.8\n",
      " -------------------------------- \n",
      "pred: 945.6\n",
      "real: 905.7\n",
      " -------------------------------- \n",
      "pred: 1132.9\n",
      "real: 1186.3\n",
      " -------------------------------- \n",
      "pred: 743.7\n",
      "real: 891.1\n",
      " -------------------------------- \n",
      "pred: 542.6\n",
      "real: 486.9\n",
      " -------------------------------- \n",
      "pred: 949.9\n",
      "real: 1278.5\n",
      " -------------------------------- \n",
      "pred: 931.1\n",
      "real: 819.2\n",
      " -------------------------------- \n",
      "pred: 995.1\n",
      "real: 868.7\n",
      " -------------------------------- \n",
      "pred: 833.3\n",
      "real: 440.1\n",
      " -------------------------------- \n",
      "pred: 703.0\n",
      "real: 638.3\n",
      " -------------------------------- \n",
      "pred: 593.9\n",
      "real: 513.4\n",
      " -------------------------------- \n",
      "pred: 815.6\n",
      "real: 665.8\n",
      " -------------------------------- \n",
      "pred: 960.8\n",
      "real: 551.8\n",
      " -------------------------------- \n",
      "pred: 713.0\n",
      "real: 464.6\n",
      " -------------------------------- \n",
      "pred: 1433.6\n",
      "real: 1353.1\n",
      " -------------------------------- \n",
      "pred: 564.2\n",
      "real: 740.1\n",
      " -------------------------------- \n",
      "pred: 537.2\n",
      "real: 867.1\n",
      " -------------------------------- \n",
      "pred: 1209.9\n",
      "real: 1204.6\n",
      " -------------------------------- \n",
      "pred: 462.9\n",
      "real: 476.2\n",
      " -------------------------------- \n",
      "pred: 768.9\n",
      "real: 465.9\n",
      " -------------------------------- \n",
      "pred: 944.3\n",
      "real: 1492.4\n",
      " -------------------------------- \n",
      "pred: 1159.5\n",
      "real: 1117.1\n",
      " -------------------------------- \n",
      "pred: 755.6\n",
      "real: 923.0\n",
      " -------------------------------- \n",
      "pred: 843.3\n",
      "real: 938.1\n",
      " -------------------------------- \n",
      "pred: 164.2\n",
      "real: 634.3\n",
      " -------------------------------- \n",
      "pred: 516.5\n",
      "real: 1182.5\n",
      " -------------------------------- \n",
      "pred: 886.2\n",
      "real: 953.1\n",
      " -------------------------------- \n",
      "pred: 606.5\n",
      "real: 488.1\n",
      " -------------------------------- \n",
      "pred: 877.4\n",
      "real: 823.6\n",
      " -------------------------------- \n",
      "pred: 592.5\n",
      "real: 488.1\n",
      " -------------------------------- \n",
      "pred: 1223.1\n",
      "real: 1187.9\n",
      " -------------------------------- \n",
      "pred: 1047.7\n",
      "real: 964.7\n",
      " -------------------------------- \n",
      "pred: 1420.0\n",
      "real: 1481.7\n",
      " -------------------------------- \n",
      "pred: 1245.1\n",
      "real: 1239.6\n",
      " -------------------------------- \n",
      "pred: 1337.3\n",
      "real: 755.1\n",
      " -------------------------------- \n",
      "pred: 839.3\n",
      "real: 507.7\n",
      " -------------------------------- \n",
      "pred: 833.1\n",
      "real: 934.7\n",
      " -------------------------------- \n",
      "pred: 752.6\n",
      "real: 762.5\n",
      " -------------------------------- \n",
      "pred: 537.5\n",
      "real: 495.1\n",
      " -------------------------------- \n",
      "pred: 1382.0\n",
      "real: 1128.2\n",
      " -------------------------------- \n",
      "pred: 984.6\n",
      "real: 636.5\n",
      " -------------------------------- \n",
      "pred: 1198.3\n",
      "real: 45.3\n",
      " -------------------------------- \n",
      "pred: 787.5\n",
      "real: 497.0\n",
      " -------------------------------- \n",
      "pred: 893.4\n",
      "real: 946.4\n",
      " -------------------------------- \n",
      "pred: 1337.0\n",
      "real: 778.8\n",
      " -------------------------------- \n",
      "pred: 891.7\n",
      "real: 879.3\n",
      " -------------------------------- \n",
      "pred: 911.6\n",
      "real: 1338.1\n",
      " -------------------------------- \n",
      "pred: 1023.0\n",
      "real: 1101.7\n",
      " -------------------------------- \n",
      "pred: 641.5\n",
      "real: 753.4\n",
      " -------------------------------- \n",
      "pred: 1287.8\n",
      "real: 678.4\n",
      " -------------------------------- \n",
      "pred: 1404.7\n",
      "real: 1393.7\n",
      " -------------------------------- \n",
      "pred: 1041.8\n",
      "real: 790.2\n",
      " -------------------------------- \n",
      "pred: 634.4\n",
      "real: 656.0\n",
      " -------------------------------- \n",
      "pred: 739.1\n",
      "real: 842.9\n",
      " -------------------------------- \n",
      "pred: 708.8\n",
      "real: 597.6\n",
      " -------------------------------- \n",
      "pred: 1028.1\n",
      "real: 2096.9\n",
      " -------------------------------- \n",
      "pred: 456.0\n",
      "real: 613.8\n",
      " -------------------------------- \n",
      "pred: 1114.8\n",
      "real: 1244.4\n",
      " -------------------------------- \n",
      "pred: 539.3\n",
      "real: 623.3\n",
      " -------------------------------- \n",
      "pred: 877.1\n",
      "real: 593.7\n",
      " -------------------------------- \n",
      "pred: 642.2\n",
      "real: 597.4\n",
      " -------------------------------- \n",
      "pred: 1251.1\n",
      "real: 1159.5\n",
      " -------------------------------- \n",
      "pred: 1279.0\n",
      "real: 2412.3\n",
      " -------------------------------- \n",
      "pred: 1578.4\n",
      "real: 1355.4\n",
      " -------------------------------- \n",
      "pred: 917.9\n",
      "real: 1156.9\n",
      " -------------------------------- \n",
      "pred: 967.5\n",
      "real: 504.7\n",
      " -------------------------------- \n",
      "pred: 1199.4\n",
      "real: 846.4\n",
      " -------------------------------- \n",
      "pred: 1113.6\n",
      "real: 629.9\n",
      " -------------------------------- \n",
      "pred: 959.5\n",
      "real: 1104.0\n",
      " -------------------------------- \n",
      "pred: 714.7\n",
      "real: 612.4\n",
      " -------------------------------- \n",
      "pred: 1290.4\n",
      "real: 1027.7\n",
      " -------------------------------- \n",
      "pred: 793.0\n",
      "real: 443.2\n",
      " -------------------------------- \n",
      "pred: 1021.4\n",
      "real: 569.0\n",
      " -------------------------------- \n",
      "pred: 923.7\n",
      "real: 751.8\n",
      " -------------------------------- \n",
      "pred: 635.8\n",
      "real: 1020.6\n",
      " -------------------------------- \n",
      "pred: 1095.6\n",
      "real: 1326.2\n",
      " -------------------------------- \n",
      "pred: 687.5\n",
      "real: 1302.4\n",
      " -------------------------------- \n",
      "pred: 642.2\n",
      "real: 597.4\n",
      " -------------------------------- \n",
      "pred: 940.4\n",
      "real: 942.9\n",
      " -------------------------------- \n",
      "pred: 972.5\n",
      "real: 980.2\n",
      " -------------------------------- \n",
      "pred: 468.0\n",
      "real: 647.9\n",
      " -------------------------------- \n",
      "pred: 945.8\n",
      "real: 946.4\n",
      " -------------------------------- \n",
      "pred: 524.0\n",
      "real: 475.9\n",
      " -------------------------------- \n",
      "pred: 543.4\n",
      "real: 436.1\n",
      " -------------------------------- \n",
      "pred: 875.3\n",
      "real: 1306.3\n",
      " -------------------------------- \n",
      "pred: 1201.9\n",
      "real: 1270.3\n",
      " -------------------------------- \n",
      "pred: 635.5\n",
      "real: 459.8\n",
      " -------------------------------- \n",
      "pred: 1290.4\n",
      "real: 1207.5\n",
      " -------------------------------- \n",
      "pred: 909.5\n",
      "real: 723.3\n",
      " -------------------------------- \n",
      "pred: 1018.0\n",
      "real: 1191.3\n",
      " -------------------------------- \n",
      "pred: 708.8\n",
      "real: 597.6\n",
      " -------------------------------- \n",
      "pred: 717.7\n",
      "real: 610.2\n",
      " -------------------------------- \n",
      "pred: 795.1\n",
      "real: 615.5\n",
      " -------------------------------- \n",
      "pred: 708.8\n",
      "real: 597.6\n",
      " -------------------------------- \n",
      "pred: 1037.5\n",
      "real: 641.4\n",
      " -------------------------------- \n",
      "pred: 902.7\n",
      "real: 627.3\n",
      " -------------------------------- \n",
      "pred: 482.1\n",
      "real: 439.6\n",
      " -------------------------------- \n",
      "pred: 945.6\n",
      "real: 905.7\n",
      " -------------------------------- \n",
      "pred: 731.8\n",
      "real: 2784.9\n",
      " -------------------------------- \n",
      "pred: 508.1\n",
      "real: 484.2\n",
      " -------------------------------- \n",
      "pred: 960.1\n",
      "real: 1033.0\n",
      " -------------------------------- \n",
      "pred: 1628.6\n",
      "real: 1212.4\n",
      " -------------------------------- \n",
      "pred: 507.5\n",
      "real: 463.5\n",
      " -------------------------------- \n",
      "pred: 1038.5\n",
      "real: 944.1\n",
      " -------------------------------- \n",
      "pred: 1072.4\n",
      "real: 1429.5\n",
      " -------------------------------- \n",
      "pred: 815.9\n",
      "real: 542.0\n",
      " -------------------------------- \n",
      "pred: 782.5\n",
      "real: 588.1\n",
      " -------------------------------- \n",
      "pred: 922.3\n",
      "real: 674.1\n",
      " -------------------------------- \n",
      "pred: 610.8\n",
      "real: 677.7\n",
      " -------------------------------- \n",
      "pred: 1647.5\n",
      "real: 1146.8\n",
      " -------------------------------- \n",
      "pred: 1722.4\n",
      "real: 888.3\n",
      " -------------------------------- \n",
      "pred: 371.1\n",
      "real: 533.3\n",
      " -------------------------------- \n",
      "pred: 716.4\n",
      "real: 454.8\n",
      " -------------------------------- \n",
      "pred: 778.3\n",
      "real: 529.6\n",
      " -------------------------------- \n",
      "pred: 1420.0\n",
      "real: 1481.7\n",
      " -------------------------------- \n",
      "pred: 610.2\n",
      "real: 804.7\n",
      " -------------------------------- \n",
      "pred: 1819.8\n",
      "real: 944.7\n",
      " -------------------------------- \n",
      "pred: 1457.0\n",
      "real: 1682.7\n",
      " -------------------------------- \n",
      "pred: 937.4\n",
      "real: 1201.6\n",
      " -------------------------------- \n",
      "pred: 792.0\n",
      "real: 596.2\n",
      " -------------------------------- \n",
      "pred: 642.2\n",
      "real: 597.4\n",
      " -------------------------------- \n",
      "pred: 784.8\n",
      "real: 624.8\n",
      " -------------------------------- \n",
      "pred: 658.1\n",
      "real: 888.7\n",
      " -------------------------------- \n",
      "pred: 1282.2\n",
      "real: 1109.1\n",
      " -------------------------------- \n",
      "pred: 1280.5\n",
      "real: 855.6\n",
      " -------------------------------- \n",
      "pred: 1628.6\n",
      "real: 1212.4\n",
      " -------------------------------- \n",
      "pred: 642.2\n",
      "real: 597.4\n",
      " -------------------------------- \n",
      "pred: 768.4\n",
      "real: 610.6\n",
      " -------------------------------- \n",
      "pred: 1426.7\n",
      "real: 1254.0\n",
      " -------------------------------- \n",
      "pred: 1258.1\n",
      "real: 1396.8\n",
      " -------------------------------- \n",
      "pred: 813.2\n",
      "real: 564.8\n",
      " -------------------------------- \n",
      "pred: 566.0\n",
      "real: 950.5\n",
      " -------------------------------- \n",
      "pred: 880.5\n",
      "real: 449.5\n",
      " -------------------------------- \n",
      "pred: 1024.1\n",
      "real: 501.4\n",
      " -------------------------------- \n",
      "pred: 699.3\n",
      "real: 944.6\n",
      " -------------------------------- \n",
      "pred: 1064.2\n",
      "real: 1376.7\n",
      " -------------------------------- \n",
      "pred: 647.1\n",
      "real: 1306.1\n",
      " -------------------------------- \n",
      "pred: 1514.3\n",
      "real: 1066.2\n",
      " -------------------------------- \n",
      "pred: 1233.9\n",
      "real: 1362.3\n",
      " -------------------------------- \n",
      "pred: 703.3\n",
      "real: 520.4\n",
      " -------------------------------- \n",
      "pred: 890.0\n",
      "real: 908.4\n",
      " -------------------------------- \n",
      "pred: 949.8\n",
      "real: 1072.2\n",
      " -------------------------------- \n",
      "pred: 957.3\n",
      "real: 1037.8\n",
      " -------------------------------- \n",
      "pred: 877.4\n",
      "real: 823.6\n",
      " -------------------------------- \n",
      "pred: 951.5\n",
      "real: 996.8\n",
      " -------------------------------- \n",
      "pred: 531.4\n",
      "real: 1119.2\n",
      " -------------------------------- \n",
      "pred: 757.7\n",
      "real: 563.8\n",
      " -------------------------------- \n",
      "pred: 877.4\n",
      "real: 823.6\n",
      " -------------------------------- \n",
      "pred: 741.1\n",
      "real: 699.1\n",
      " -------------------------------- \n",
      "pred: 1062.4\n",
      "real: 1185.0\n",
      " -------------------------------- \n",
      "pred: 1433.6\n",
      "real: 1353.1\n",
      " -------------------------------- \n",
      "pred: 1071.5\n",
      "real: 695.6\n",
      " -------------------------------- \n",
      "pred: 891.9\n",
      "real: 442.0\n",
      " -------------------------------- \n",
      "pred: 1082.0\n",
      "real: 710.4\n",
      " -------------------------------- \n",
      "pred: 940.5\n",
      "real: 838.1\n",
      " -------------------------------- \n",
      "pred: 1037.0\n",
      "real: 1125.3\n",
      " -------------------------------- \n",
      "pred: 732.7\n",
      "real: 737.9\n",
      " -------------------------------- \n",
      "pred: 1186.2\n",
      "real: 1215.7\n",
      " -------------------------------- \n",
      "pred: 1303.4\n",
      "real: 974.8\n",
      " -------------------------------- \n",
      "pred: 854.8\n",
      "real: 1181.3\n",
      " -------------------------------- \n",
      "pred: 1279.9\n",
      "real: 805.2\n",
      " -------------------------------- \n",
      "pred: 1116.4\n",
      "real: 1223.5\n",
      " -------------------------------- \n",
      "pred: 777.1\n",
      "real: 638.0\n",
      " -------------------------------- \n",
      "pred: 815.6\n",
      "real: 665.8\n",
      " -------------------------------- \n",
      "pred: 957.3\n",
      "real: 693.7\n",
      " -------------------------------- \n",
      "pred: 1026.2\n",
      "real: 1201.4\n",
      " -------------------------------- \n",
      "pred: 830.1\n",
      "real: 901.7\n",
      " -------------------------------- \n",
      "pred: 944.3\n",
      "real: 1492.4\n",
      " -------------------------------- \n",
      "pred: 971.5\n",
      "real: 838.4\n",
      " -------------------------------- \n",
      "pred: 957.3\n",
      "real: 693.7\n",
      " -------------------------------- \n",
      "pred: 1004.1\n",
      "real: 558.7\n",
      " -------------------------------- \n",
      "pred: 689.5\n",
      "real: 475.9\n",
      " -------------------------------- \n",
      "pred: 823.7\n",
      "real: 564.4\n",
      " -------------------------------- \n",
      "pred: 642.2\n",
      "real: 597.4\n",
      " -------------------------------- \n",
      "pred: 1457.6\n",
      "real: 1133.9\n",
      " -------------------------------- \n",
      "pred: 1071.9\n",
      "real: 888.3\n",
      " -------------------------------- \n",
      "pred: 908.1\n",
      "real: 997.5\n",
      " -------------------------------- \n",
      "pred: 1560.2\n",
      "real: 2263.5\n",
      " -------------------------------- \n",
      "pred: 652.4\n",
      "real: 1007.9\n",
      " -------------------------------- \n",
      "pred: 959.5\n",
      "real: 781.6\n",
      " -------------------------------- \n",
      "pred: 1079.0\n",
      "real: 544.2\n",
      " -------------------------------- \n",
      "pred: 945.6\n",
      "real: 905.7\n",
      " -------------------------------- \n",
      "pred: 1041.7\n",
      "real: 653.1\n",
      " -------------------------------- \n",
      "pred: 883.1\n",
      "real: 891.1\n",
      " -------------------------------- \n",
      "pred: 1233.9\n",
      "real: 541.3\n",
      " -------------------------------- \n",
      "pred: 1099.8\n",
      "real: 1112.3\n",
      " -------------------------------- \n",
      "pred: 996.3\n",
      "real: 1706.1\n",
      " -------------------------------- \n",
      "pred: 940.5\n",
      "real: 838.1\n",
      " -------------------------------- \n",
      "pred: 1661.6\n",
      "real: 968.8\n",
      " -------------------------------- \n",
      "pred: 844.6\n",
      "real: 828.6\n",
      " -------------------------------- \n",
      "pred: 940.4\n",
      "real: 942.9\n",
      " -------------------------------- \n",
      "pred: 877.4\n",
      "real: 823.6\n",
      " -------------------------------- \n",
      "pred: 920.5\n",
      "real: 979.3\n",
      " -------------------------------- \n",
      "pred: 668.9\n",
      "real: 469.8\n",
      " -------------------------------- \n",
      "pred: 1329.9\n",
      "real: 1205.7\n",
      " -------------------------------- \n",
      "pred: 1435.7\n",
      "real: 1315.9\n",
      " -------------------------------- \n",
      "pred: 603.1\n",
      "real: 797.4\n",
      " -------------------------------- \n",
      "pred: 903.6\n",
      "real: 1423.7\n",
      " -------------------------------- \n",
      "pred: 920.8\n",
      "real: 1388.1\n",
      " -------------------------------- \n",
      "pred: 1154.3\n",
      "real: 804.9\n",
      " -------------------------------- \n",
      "pred: 1395.9\n",
      "real: 1059.4\n",
      " -------------------------------- \n",
      "pred: 912.5\n",
      "real: 1038.1\n",
      " -------------------------------- \n",
      "pred: 747.8\n",
      "real: 501.7\n",
      " -------------------------------- \n",
      "pred: 651.3\n",
      "real: 743.7\n",
      " -------------------------------- \n",
      "pred: 655.1\n",
      "real: 512.1\n",
      " -------------------------------- \n",
      "pred: 1117.1\n",
      "real: 1453.0\n",
      " -------------------------------- \n",
      "pred: 871.6\n",
      "real: 925.5\n",
      " -------------------------------- \n",
      "pred: 981.7\n",
      "real: 1255.2\n",
      " -------------------------------- \n",
      "pred: 887.2\n",
      "real: 764.8\n",
      " -------------------------------- \n",
      "pred: 1044.3\n",
      "real: 649.0\n",
      " -------------------------------- \n",
      "pred: 1262.6\n",
      "real: 641.0\n",
      " -------------------------------- \n",
      "pred: 1114.8\n",
      "real: 1244.4\n",
      " -------------------------------- \n",
      "pred: 1294.9\n",
      "real: 855.6\n",
      " -------------------------------- \n",
      "pred: 629.3\n",
      "real: 848.0\n",
      " -------------------------------- \n",
      "pred: 668.6\n",
      "real: 521.4\n",
      " -------------------------------- \n",
      "pred: 653.2\n",
      "real: 1663.4\n",
      " -------------------------------- \n",
      "pred: 492.3\n",
      "real: 522.1\n",
      " -------------------------------- \n",
      "pred: 849.8\n",
      "real: 1028.6\n",
      " -------------------------------- \n",
      "pred: 684.7\n",
      "real: 1519.3\n",
      " -------------------------------- \n",
      "pred: 659.7\n",
      "real: 1260.0\n",
      " -------------------------------- \n",
      "pred: 779.8\n",
      "real: 659.7\n",
      " -------------------------------- \n",
      "pred: 658.6\n",
      "real: 908.5\n",
      " -------------------------------- \n",
      "pred: 634.7\n",
      "real: 610.2\n",
      " -------------------------------- \n",
      "pred: 697.5\n",
      "real: 1152.8\n",
      " -------------------------------- \n",
      "pred: 637.2\n",
      "real: 479.7\n",
      " -------------------------------- \n",
      "pred: 1038.2\n",
      "real: 1022.1\n",
      " -------------------------------- \n",
      "pred: 984.7\n",
      "real: 1805.4\n",
      " -------------------------------- \n",
      "pred: 890.0\n",
      "real: 908.4\n",
      " -------------------------------- \n",
      "pred: 509.8\n",
      "real: 449.4\n",
      " -------------------------------- \n",
      "pred: 3863.3\n",
      "real: 976.4\n",
      " -------------------------------- \n",
      "pred: 951.4\n",
      "real: 1309.0\n",
      " -------------------------------- \n",
      "pred: 1041.3\n",
      "real: 1494.2\n",
      " -------------------------------- \n",
      "pred: 944.3\n",
      "real: 1492.4\n",
      " -------------------------------- \n",
      "pred: 1061.3\n",
      "real: 638.7\n",
      " -------------------------------- \n",
      "pred: 720.5\n",
      "real: 627.0\n",
      " -------------------------------- \n",
      "pred: 1124.9\n",
      "real: 463.1\n",
      " -------------------------------- \n",
      "pred: 711.0\n",
      "real: 663.9\n",
      " -------------------------------- \n",
      "pred: 1290.4\n",
      "real: 1207.5\n",
      " -------------------------------- \n",
      "pred: 818.5\n",
      "real: 959.4\n",
      " -------------------------------- \n",
      "pred: 821.9\n",
      "real: 807.4\n",
      " -------------------------------- \n",
      "pred: 868.8\n",
      "real: 913.3\n",
      " -------------------------------- \n",
      "pred: 859.8\n",
      "real: 826.7\n",
      " -------------------------------- \n",
      "pred: 753.0\n",
      "real: 688.5\n",
      " -------------------------------- \n",
      "pred: 743.9\n",
      "real: 449.4\n",
      " -------------------------------- \n",
      "pred: 856.0\n",
      "real: 937.5\n",
      " -------------------------------- \n",
      "pred: 1290.4\n",
      "real: 1207.5\n",
      " -------------------------------- \n",
      "pred: 1690.3\n",
      "real: 996.9\n",
      " -------------------------------- \n",
      "pred: 1475.8\n",
      "real: 647.7\n",
      " -------------------------------- \n",
      "pred: 945.6\n",
      "real: 905.7\n",
      " -------------------------------- \n",
      "pred: 836.7\n",
      "real: 804.3\n",
      " -------------------------------- \n",
      "pred: 672.8\n",
      "real: 553.7\n",
      " -------------------------------- \n",
      "pred: 610.8\n",
      "real: 677.7\n",
      " -------------------------------- \n",
      "pred: 861.3\n",
      "real: 733.3\n",
      " -------------------------------- \n",
      "pred: 742.1\n",
      "real: 577.8\n",
      " -------------------------------- \n",
      "pred: 1136.2\n",
      "real: 1194.6\n",
      " -------------------------------- \n",
      "pred: 923.7\n",
      "real: 751.8\n",
      " -------------------------------- \n",
      "pred: 873.2\n",
      "real: 935.4\n",
      " -------------------------------- \n",
      "pred: 894.9\n",
      "real: 741.5\n",
      " -------------------------------- \n",
      "pred: 669.7\n",
      "real: 443.7\n",
      " -------------------------------- \n",
      "pred: 901.4\n",
      "real: 1025.0\n",
      " -------------------------------- \n",
      "pred: 1118.3\n",
      "real: 913.7\n",
      " -------------------------------- \n",
      "pred: 650.6\n",
      "real: 877.9\n",
      " -------------------------------- \n",
      "pred: 645.0\n",
      "real: 601.3\n",
      " -------------------------------- \n",
      "pred: 1413.3\n",
      "real: 762.3\n",
      " -------------------------------- \n",
      "pred: 1209.9\n",
      "real: 1204.6\n",
      " -------------------------------- \n",
      "pred: 820.2\n",
      "real: 1168.6\n",
      " -------------------------------- \n",
      "pred: 684.7\n",
      "real: 1519.3\n",
      " -------------------------------- \n",
      "pred: 1062.4\n",
      "real: 1185.0\n",
      " -------------------------------- \n",
      "pred: 588.5\n",
      "real: 1026.7\n",
      " -------------------------------- \n",
      "pred: 789.3\n",
      "real: 618.7\n",
      " -------------------------------- \n",
      "pred: 877.4\n",
      "real: 823.6\n",
      " -------------------------------- \n",
      "pred: 1280.4\n",
      "real: 1276.2\n",
      " -------------------------------- \n",
      "pred: 1018.4\n",
      "real: 720.5\n",
      " -------------------------------- \n",
      "pred: 1254.9\n",
      "real: 1010.9\n",
      " -------------------------------- \n",
      "pred: 1052.0\n",
      "real: 1151.7\n",
      " -------------------------------- \n",
      "pred: 999.8\n",
      "real: 1024.8\n",
      " -------------------------------- \n",
      "pred: 1461.5\n",
      "real: 1408.3\n",
      " -------------------------------- \n",
      "pred: 1008.0\n",
      "real: 565.1\n",
      " -------------------------------- \n",
      "pred: 846.8\n",
      "real: 901.8\n",
      " -------------------------------- \n",
      "pred: 774.7\n",
      "real: 1152.3\n",
      " -------------------------------- \n",
      "pred: 822.1\n",
      "real: 448.1\n",
      " -------------------------------- \n",
      "pred: 1096.9\n",
      "real: 987.7\n",
      " -------------------------------- \n",
      "pred: 946.0\n",
      "real: 894.7\n",
      " -------------------------------- \n",
      "pred: 951.5\n",
      "real: 1064.0\n",
      " -------------------------------- \n",
      "pred: 1461.5\n",
      "real: 1408.3\n",
      " -------------------------------- \n",
      "pred: 932.2\n",
      "real: 964.3\n",
      " -------------------------------- \n",
      "pred: 1329.9\n",
      "real: 1205.7\n",
      " -------------------------------- \n",
      "pred: 937.4\n",
      "real: 1201.6\n",
      " -------------------------------- \n",
      "pred: 1418.1\n",
      "real: 1152.0\n",
      " -------------------------------- \n",
      "pred: 489.8\n",
      "real: 424.2\n",
      " -------------------------------- \n",
      "pred: 646.2\n",
      "real: 860.5\n",
      " -------------------------------- \n",
      "pred: 892.6\n",
      "real: 675.6\n",
      " -------------------------------- \n",
      "pred: 937.4\n",
      "real: 1201.6\n",
      " -------------------------------- \n",
      "pred: 901.8\n",
      "real: 532.9\n",
      " -------------------------------- \n",
      "pred: 946.0\n",
      "real: 894.7\n",
      " -------------------------------- \n",
      "pred: 992.3\n",
      "real: 1563.5\n",
      " -------------------------------- \n",
      "pred: 1071.3\n",
      "real: 1175.9\n",
      " -------------------------------- \n",
      "pred: 892.4\n",
      "real: 576.1\n",
      " -------------------------------- \n",
      "pred: 945.8\n",
      "real: 946.4\n",
      " -------------------------------- \n",
      "pred: 994.2\n",
      "real: 646.4\n",
      " -------------------------------- \n",
      "pred: 957.3\n",
      "real: 693.7\n",
      " -------------------------------- \n",
      "pred: 659.3\n",
      "real: 563.1\n",
      " -------------------------------- \n",
      "pred: 1226.9\n",
      "real: 1657.9\n",
      " -------------------------------- \n",
      "pred: 894.6\n",
      "real: 590.9\n",
      " -------------------------------- \n",
      "pred: 946.0\n",
      "real: 894.7\n",
      " -------------------------------- \n",
      "pred: 1032.3\n",
      "real: 1201.4\n",
      " -------------------------------- \n",
      "pred: 802.1\n",
      "real: 713.8\n",
      " -------------------------------- \n",
      "pred: 867.1\n",
      "real: 965.1\n",
      " -------------------------------- \n",
      "pred: 722.3\n",
      "real: 580.7\n",
      " -------------------------------- \n",
      "pred: 945.8\n",
      "real: 946.4\n",
      " -------------------------------- \n",
      "pred: 612.0\n",
      "real: 1175.6\n",
      " -------------------------------- \n",
      "pred: 874.6\n",
      "real: 1225.3\n",
      " -------------------------------- \n",
      "pred: 725.2\n",
      "real: 532.8\n",
      " -------------------------------- \n",
      "pred: 1170.1\n",
      "real: 1151.2\n",
      " -------------------------------- \n",
      "pred: 1025.2\n",
      "real: 930.2\n",
      " -------------------------------- \n",
      "pred: 926.1\n",
      "real: 715.7\n",
      " -------------------------------- \n",
      "pred: 922.6\n",
      "real: 822.6\n",
      " -------------------------------- \n",
      "pred: 1098.7\n",
      "real: 1097.2\n",
      " -------------------------------- \n",
      "pred: 822.2\n",
      "real: 1138.7\n",
      " -------------------------------- \n",
      "pred: 853.7\n",
      "real: 434.4\n",
      " -------------------------------- \n",
      "pred: 945.5\n",
      "real: 1000.0\n",
      " -------------------------------- \n",
      "pred: 708.8\n",
      "real: 597.6\n",
      " -------------------------------- \n",
      "pred: 711.2\n",
      "real: 480.6\n",
      " -------------------------------- \n",
      "pred: 1091.2\n",
      "real: 1104.1\n",
      " -------------------------------- \n",
      "pred: 813.5\n",
      "real: 813.1\n",
      " -------------------------------- \n",
      "pred: 789.3\n",
      "real: 618.7\n",
      " -------------------------------- \n",
      "pred: 846.2\n",
      "real: 1236.6\n",
      " -------------------------------- \n",
      "pred: 1127.8\n",
      "real: 839.7\n",
      " -------------------------------- \n",
      "pred: 946.0\n",
      "real: 894.7\n",
      " -------------------------------- \n",
      "pred: 971.5\n",
      "real: 942.2\n",
      " -------------------------------- \n",
      "pred: 897.9\n",
      "real: 1012.0\n",
      " -------------------------------- \n",
      "pred: 751.6\n",
      "real: 871.9\n",
      " -------------------------------- \n",
      "pred: 893.9\n",
      "real: 1347.0\n",
      " -------------------------------- \n",
      "pred: 930.5\n",
      "real: 598.8\n",
      " -------------------------------- \n",
      "pred: 1020.5\n",
      "real: 1261.3\n",
      " -------------------------------- \n",
      "pred: 946.0\n",
      "real: 894.7\n",
      " -------------------------------- \n",
      "pred: 1254.0\n",
      "real: 930.8\n",
      " -------------------------------- \n",
      "pred: 707.2\n",
      "real: 380.5\n",
      " -------------------------------- \n",
      "pred: 2075.3\n",
      "real: 1187.2\n",
      " -------------------------------- \n",
      "pred: 774.2\n",
      "real: 1301.1\n",
      " -------------------------------- \n",
      "pred: 873.2\n",
      "real: 935.4\n",
      " -------------------------------- \n",
      "pred: 642.2\n",
      "real: 597.4\n",
      " -------------------------------- \n",
      "pred: 639.6\n",
      "real: 1028.9\n",
      " -------------------------------- \n",
      "pred: 529.0\n",
      "real: 961.1\n",
      " -------------------------------- \n",
      "pred: 666.2\n",
      "real: 512.7\n",
      " -------------------------------- \n",
      "pred: 945.6\n",
      "real: 905.7\n",
      " -------------------------------- \n",
      "pred: 531.3\n",
      "real: 499.7\n",
      " -------------------------------- \n",
      "pred: 742.1\n",
      "real: 577.8\n",
      " -------------------------------- \n",
      "pred: 358.9\n",
      "real: 486.9\n",
      " -------------------------------- \n",
      "pred: 856.0\n",
      "real: 937.5\n",
      " -------------------------------- \n",
      "pred: 845.8\n",
      "real: 445.7\n",
      " -------------------------------- \n",
      "pred: 944.3\n",
      "real: 1492.4\n",
      " -------------------------------- \n",
      "pred: 865.7\n",
      "real: 740.3\n",
      " -------------------------------- \n",
      "pred: 532.9\n",
      "real: 499.0\n",
      " -------------------------------- \n",
      "pred: 1018.6\n",
      "real: 1094.8\n",
      " -------------------------------- \n",
      "pred: 630.3\n",
      "real: 477.5\n",
      " -------------------------------- \n",
      "pred: 1023.9\n",
      "real: 780.0\n",
      " -------------------------------- \n",
      "pred: 873.2\n",
      "real: 935.4\n",
      " -------------------------------- \n",
      "pred: 829.6\n",
      "real: 668.4\n",
      " -------------------------------- \n",
      "pred: 890.0\n",
      "real: 908.4\n",
      " -------------------------------- \n",
      "pred: 788.7\n",
      "real: 459.6\n",
      " -------------------------------- \n",
      "pred: 1060.2\n",
      "real: 1151.7\n",
      " -------------------------------- \n",
      "pred: 838.0\n",
      "real: 817.0\n",
      " -------------------------------- \n",
      "pred: 970.0\n",
      "real: 1370.4\n",
      " -------------------------------- \n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader_test:\n",
    "    x, y = batch    \n",
    "    x = x.float()\n",
    "    y = y.float() \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(x, edge_index)\n",
    "        \n",
    "        prediction = float(out[0][0] * 1e4)\n",
    "        real = float(y[0] * 1e4)\n",
    "        print(f\"pred: {prediction:.1f}\")\n",
    "        print(f\"real: {real:.1f}\")\n",
    "        print(\" -------------------------------- \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
